{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "543cf45a",
   "metadata": {},
   "source": [
    "# 1. Notebook Setup\n",
    "# Title, assignment info, and markdown overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "044f2c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "import csv\n",
    "from typing import Union\n",
    "from typing import Callable\n",
    "import time\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48957c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_file_path = \"data/Mappings.csv\"\n",
    "employees_file_path = \"data/Employee.csv\"\n",
    "tasks_file_path = \"data/Tasks.csv\"\n",
    "rd.seed(20)\n",
    "\n",
    "# Data Classes\n",
    "class Task:\n",
    "    time:int =0\n",
    "    difficulty:int =0\n",
    "    deadline:int = 0\n",
    "    required_skill:str = ''\n",
    "    def __str__(self):\n",
    "        return f\"T Time:{self.time} Difficulty:{self.difficulty} Deadline:{self.deadline} requiredSkill:{self.required_skill}\"\n",
    "\n",
    "class Employee:\n",
    "    available_hours:int = 0\n",
    "    skill_level:int = 4\n",
    "    skills:list[str]=['']\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"E AH:{self.available_hours} Skill-Level:{self.skill_level} Skills:{self.skills}\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ea3c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O\n",
    "\"\"\"\n",
    "Task csv:\n",
    "\n",
    "'ID', 'Time (hrs)', 'Difficulty', 'Deadline (hrs)', 'Required Skill'\n",
    "\n",
    "Employee csv:\n",
    "'Employee ID', 'Available Hrs', 'Skill Level', 'Skills'\n",
    "\n",
    "\n",
    "Using an adjancey list instead of an adjacney matrix!\n",
    "\n",
    "Since the input vector is 10 mappings x 11 features. Including a unique assignment penalty is redundant since a task mapped to 2 employees will create (10+1) mappings which doesn't work for the network input layer!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    num_of_tasks =10\n",
    "    num_of_employees = 5\n",
    "    tasks:list[Task]=[]\n",
    "    employees:list[Employee]=[]\n",
    "    \n",
    "    def load_tasks(self,fileName=tasks_file_path):\n",
    "        tasks =[]\n",
    "        with open(fileName,'r') as csvfile:\n",
    "            taskReader = csv.reader(csvfile)\n",
    "            next(taskReader)\n",
    "            for taskArr in taskReader:\n",
    "                newT = Task()\n",
    "                newT.time=int(taskArr[1])\n",
    "                newT.difficulty= int(taskArr[2])\n",
    "                newT.deadline = int(taskArr[3])\n",
    "                newT.required_skill= taskArr[4]\n",
    "                tasks.append(newT)\n",
    "        self.tasks= tasks\n",
    "\n",
    "    def load_employees(self,fileName=employees_file_path):\n",
    "        employees=[]\n",
    "        with open(fileName,'r') as csvfile:\n",
    "            employeeReader = csv.reader(csvfile)\n",
    "            next(employeeReader)\n",
    "            for employeeArr in employeeReader:\n",
    "                newEmployee = Employee()\n",
    "                newEmployee.available_hours=int(employeeArr[1])\n",
    "                newEmployee.skill_level= int(employeeArr[2])\n",
    "                skills = employeeArr[3].split(',')\n",
    "                newEmployee.skills = skills\n",
    "                employees.append(newEmployee)\n",
    "        self.employees = employees\n",
    "    def loadAll(self):\n",
    "        self.load_employees()\n",
    "        self.load_tasks()\n",
    "    \n",
    "\n",
    "\n",
    "class MappingHandler:\n",
    "    def __init__(self,tasks:list[Task],employees:list[Employee],num_of_mappings=100):\n",
    "        self.tasks:list[Task]= tasks\n",
    "        self.employees:list[Employee] = employees\n",
    "        self.num_of_mappings = num_of_mappings\n",
    "        self.mappings:list[list[int]] = []\n",
    "        self.costs:list[float]=[]\n",
    "\n",
    "\n",
    "    def __costFunction(self,Mapping:list[int]):\n",
    "        w=0.2\n",
    "        overload = 0\n",
    "        skill_mismatch = 0\n",
    "        difficulty_violation = 0\n",
    "        deadline_violation = 0\n",
    "        unique_assignment = 0\n",
    "        employee_task_adj_list:list[list[Task]]=[[] for _ in range(len(self.employees))]\n",
    "\n",
    "        for i in range(len(Mapping)):\n",
    "            taskId=i\n",
    "            task = self.tasks[taskId]\n",
    "            num_of_employees_assigned = 0\n",
    "            employeeId = Mapping[i]\n",
    "            num_of_employees_assigned+=1\n",
    "            employee = self.employees[employeeId]\n",
    "            employee_task_adj_list[employeeId].append(task)\n",
    "            # skill mismatch violation\n",
    "            if task.required_skill not in employee.skills:\n",
    "                skill_mismatch+=1\n",
    "            if task.difficulty> employee.skill_level:\n",
    "                difficulty_violation+= 1\n",
    "            unique_assignment+= max(0,num_of_employees_assigned-1)\n",
    "\n",
    "\n",
    "\n",
    "        for employeeId in range(len(employee_task_adj_list)):\n",
    "            employee = self.employees[employeeId]\n",
    "            sortedEmployeeTasks = employee_task_adj_list[employeeId]\n",
    "            sortedEmployeeTasks.sort(key= lambda x:x.time)\n",
    "            sumHours=0\n",
    "            finishTime=0\n",
    "            for t in sortedEmployeeTasks:\n",
    "                sumHours+=t.time\n",
    "                finishTime+= t.time\n",
    "                deadline_violation+= max(0,finishTime - t.deadline)\n",
    "            overload+= max(0,sumHours- employee.available_hours) \n",
    "        total_penalty = overload+skill_mismatch+unique_assignment+deadline_violation+ difficulty_violation\n",
    "        return round(total_penalty*w,3)\n",
    "    def __reset(self):\n",
    "        self.mappings=[]\n",
    "        self.costs=[]\n",
    "\n",
    "\n",
    "    def generateMappings(self):\n",
    "        unique = set()\n",
    "        self.__reset()\n",
    "        while len(unique) < self.num_of_mappings:\n",
    "            mapping:list[int] = [0 for _ in self.tasks]         # Create a mapping for each iteration. Using an adjlist format\n",
    "            # possible_assignments = [i for i in range(len(employees))]  #list of possible assignments\n",
    "            for taskId in range(len(self.tasks)):\n",
    "                rand_employee =rd.randint(0,len(self.employees)-1)\n",
    "                mapping[taskId]=rand_employee\n",
    "            string = str(mapping)\n",
    "            if string not in unique: # ensure no duplicates in data generation\n",
    "                unique.add(string)\n",
    "                self.mappings.append(mapping)\n",
    "                self.costs.append(self.__costFunction(mapping))\n",
    "    \n",
    "\n",
    "    def readCSV(self,fileName=mapping_file_path):\n",
    "        self.__reset()\n",
    "        with open(fileName,'r') as csvfile:\n",
    "            mappingReader = csv.reader(csvfile)\n",
    "            next(mappingReader)\n",
    "            for line in mappingReader:\n",
    "                newMapping:list[int]=[]\n",
    "                for v in line:\n",
    "                    if v.isdigit():\n",
    "                        newMapping.append(int(v))\n",
    "                    else:\n",
    "                        self.costs.append(float(v))\n",
    "\n",
    "                self.mappings.append(newMapping)\n",
    "\n",
    "\n",
    "    def writeCSV(self,filepath =mapping_file_path):\n",
    "        with open(filepath,'w',newline='') as csvfile:\n",
    "                mappingWriter = csv.writer(csvfile,delimiter=',')\n",
    "                mappingWriter.writerow([\"T1\",'T2','T3','T4','T5','T6','T7','T8','T9','T10','Penalty'])\n",
    "                for i in range(len(self.mappings)):\n",
    "                    row=[]\n",
    "                    row.extend(self.mappings[i])\n",
    "                    row.append(f\"{self.costs[i]}\")\n",
    "                    mappingWriter.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def testDataGen():\n",
    "    dl = DataLoader()\n",
    "    dl.loadAll()\n",
    "    mappingloader = MappingHandler(dl.tasks,dl.employees)\n",
    "    mappingloader.generateMappings()\n",
    "    mappingloader.writeCSV('data/Mappings.csv')\n",
    "\n",
    "\n",
    "testDataGen()\n",
    "\n",
    "\n",
    "# def TestDataGen():\n",
    "\n",
    "#     all_mappings = mappingGenerator(task,employees)\n",
    "#     for mapping in all_mappings:\n",
    "#         cost = costFunction(mapping,task,employees)\n",
    "#         print(cost)\n",
    "\n",
    "# def verifyCost():\n",
    "#     mapping=[[2],[3],[1],[4],[2],[5],[1],[3],[5],[4]]\n",
    "    \n",
    "#     for i in range(len(mapping)):\n",
    "#         mapping[i][0]-=1\n",
    "#     employees,tasks = DataLoader().loadAll()\n",
    "#     # for t in tasks:\n",
    "#     #     print(t)\n",
    "    \n",
    "#     for e in employees:\n",
    "#         print(e)\n",
    "#     print(costFunction(mapping,tasks,employees))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18263f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    0    1    2    3    4    5    6    7    8    9    ...  101  102  103  104  \\\n",
       " 0     4    3    8    1    0    0    8    3    1    0  ...   11    0    0    1   \n",
       " 1     4    3    8    1    0    0    8    3    1    0  ...   11    0    0    1   \n",
       " 2     4    3    8    1    0    0   10    4    1    0  ...   11    0    0    1   \n",
       " 3     4    3    8    1    0    0   10    4    1    0  ...   11    0    0    1   \n",
       " 4     4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " ..  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       " 65    4    3    8    1    0    0   15    7    0    1  ...   11    0    0    1   \n",
       " 66    4    3    8    1    0    0    8    3    1    0  ...   11    0    0    1   \n",
       " 67    4    3    8    1    0    0   10    4    1    0  ...   11    0    0    1   \n",
       " 68    4    3    8    1    0    0   12    6    1    1  ...   11    0    0    1   \n",
       " 69    4    3    8    1    0    0   15    7    0    1  ...   11    0    0    1   \n",
       " \n",
       "     105  106  107  108  109  110  \n",
       " 0    10    4    1    0    1  3.6  \n",
       " 1     9    5    1    0    1  3.8  \n",
       " 2     9    5    1    0    1  3.4  \n",
       " 3     9    5    1    0    1  3.0  \n",
       " 4    10    4    1    0    1  4.6  \n",
       " ..  ...  ...  ...  ...  ...  ...  \n",
       " 65   12    6    1    1    1  5.0  \n",
       " 66   12    6    1    1    1  7.0  \n",
       " 67   12    6    1    1    1  4.4  \n",
       " 68   15    7    0    1    1  5.6  \n",
       " 69   10    4    1    0    1  3.0  \n",
       " \n",
       " [70 rows x 111 columns],\n",
       "     0    1    2    3    4    5    6    7    8    9    ...  101  102  103  104  \\\n",
       " 70    4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " 71    4    3    8    1    0    0   12    6    1    1  ...   11    0    0    1   \n",
       " 72    4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " 73    4    3    8    1    0    0   10    4    1    0  ...   11    0    0    1   \n",
       " 74    4    3    8    1    0    0    8    3    1    0  ...   11    0    0    1   \n",
       " 75    4    3    8    1    0    0   10    4    1    0  ...   11    0    0    1   \n",
       " 76    4    3    8    1    0    0   15    7    0    1  ...   11    0    0    1   \n",
       " 77    4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " 78    4    3    8    1    0    0   12    6    1    1  ...   11    0    0    1   \n",
       " 79    4    3    8    1    0    0   10    4    1    0  ...   11    0    0    1   \n",
       " 80    4    3    8    1    0    0    8    3    1    0  ...   11    0    0    1   \n",
       " 81    4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " 82    4    3    8    1    0    0   12    6    1    1  ...   11    0    0    1   \n",
       " 83    4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " 84    4    3    8    1    0    0   10    4    1    0  ...   11    0    0    1   \n",
       " \n",
       "     105  106  107  108  109  110  \n",
       " 70   10    4    1    0    1  4.0  \n",
       " 71    8    3    1    0    0  2.2  \n",
       " 72    9    5    1    0    1  6.4  \n",
       " 73    8    3    1    0    0  7.8  \n",
       " 74   10    4    1    0    1  5.4  \n",
       " 75    9    5    1    0    1  2.6  \n",
       " 76   12    6    1    1    1  2.8  \n",
       " 77    9    5    1    0    1  3.8  \n",
       " 78   10    4    1    0    1  8.0  \n",
       " 79   10    4    1    0    1  5.8  \n",
       " 80   12    6    1    1    1  7.0  \n",
       " 81   12    6    1    1    1  5.8  \n",
       " 82   12    6    1    1    1  2.4  \n",
       " 83   15    7    0    1    1  2.8  \n",
       " 84    8    3    1    0    0  2.0  \n",
       " \n",
       " [15 rows x 111 columns],\n",
       "     0    1    2    3    4    5    6    7    8    9    ...  101  102  103  104  \\\n",
       " 85    4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " 86    4    3    8    1    0    0   10    4    1    0  ...   11    0    0    1   \n",
       " 87    4    3    8    1    0    0    8    3    1    0  ...   11    0    0    1   \n",
       " 88    4    3    8    1    0    0   15    7    0    1  ...   11    0    0    1   \n",
       " 89    4    3    8    1    0    0    8    3    1    0  ...   11    0    0    1   \n",
       " 90    4    3    8    1    0    0   12    6    1    1  ...   11    0    0    1   \n",
       " 91    4    3    8    1    0    0   12    6    1    1  ...   11    0    0    1   \n",
       " 92    4    3    8    1    0    0   15    7    0    1  ...   11    0    0    1   \n",
       " 93    4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " 94    4    3    8    1    0    0   15    7    0    1  ...   11    0    0    1   \n",
       " 95    4    3    8    1    0    0   15    7    0    1  ...   11    0    0    1   \n",
       " 96    4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " 97    4    3    8    1    0    0   12    6    1    1  ...   11    0    0    1   \n",
       " 98    4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " 99    4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " \n",
       "     105  106  107  108  109  110  \n",
       " 85   12    6    1    1    1  5.6  \n",
       " 86    8    3    1    0    0  4.6  \n",
       " 87    9    5    1    0    1  1.8  \n",
       " 88    8    3    1    0    0  7.0  \n",
       " 89   15    7    0    1    1  5.2  \n",
       " 90   10    4    1    0    1  3.2  \n",
       " 91   15    7    0    1    1  2.2  \n",
       " 92   12    6    1    1    1  2.2  \n",
       " 93   12    6    1    1    1  3.0  \n",
       " 94    8    3    1    0    0  4.4  \n",
       " 95   10    4    1    0    1  3.4  \n",
       " 96   10    4    1    0    1  7.2  \n",
       " 97   15    7    0    1    1  5.2  \n",
       " 98    8    3    1    0    0  6.8  \n",
       " 99    8    3    1    0    0  3.2  \n",
       " \n",
       " [15 rows x 111 columns])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Preprocessing\n",
    "# returns a 3-element vector\n",
    "def one_hot_encode_skill(skillsVector):\n",
    "    skills= ['A','B','C']\n",
    "    hot_encoding=[]\n",
    "    j=0\n",
    "    for i in range(len(skills)):\n",
    "        if j< len(skillsVector)and  skills[i] == skillsVector[j]:\n",
    "            j+=1\n",
    "            hot_encoding.append(1)\n",
    "        else:\n",
    "            hot_encoding.append(0)\n",
    "    return hot_encoding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def construct_input_vector(mappingloader:MappingHandler):\n",
    "    featureVector = []\n",
    "\n",
    "    for idx,mp in  enumerate(mappingloader.mappings):\n",
    "        vector=[]\n",
    "        for i in range(len(mp)):\n",
    "\n",
    "            task = mappingloader.tasks[i]\n",
    "            employee = mappingloader.employees[mp[i]]\n",
    "            vector.extend([task.time,task.difficulty,task.deadline])\n",
    "            vector.extend(one_hot_encode_skill(task.required_skill))\n",
    "            vector.extend([employee.available_hours,employee.skill_level])\n",
    "            vector.extend(one_hot_encode_skill( employee.skills))\n",
    "        vector.append(mappingloader.costs[idx])\n",
    "        featureVector.append(vector)\n",
    "        \n",
    "    df = DataFrame(featureVector)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_train_val_test(df:DataFrame):\n",
    "    df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    train_size = int(0.7 * len(df_shuffled))\n",
    "    val_size = int(0.15 * len(df_shuffled))\n",
    "\n",
    "    train_df = df_shuffled[:train_size]\n",
    "    val_df = df_shuffled[train_size:train_size + val_size]\n",
    "    test_df = df_shuffled[train_size + val_size:]\n",
    "    return  train_df,val_df,test_df\n",
    "\n",
    "def split_x_y(df:DataFrame):\n",
    "\n",
    "    x = df.iloc[:, :-1]\n",
    "\n",
    "    y = df.iloc[:, -1]\n",
    "    return x,y\n",
    "\n",
    "\n",
    "def create_batches(data:DataFrame, batch_size:int):\n",
    "    batches=[]\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batches.append(data[i:i + batch_size])\n",
    "    return batches\n",
    "\n",
    "\n",
    "def pre_process():\n",
    "    dl = DataLoader()\n",
    "    dl.loadAll()\n",
    "    mappingloader = MappingHandler(dl.tasks,dl.employees)\n",
    "    mappingloader.readCSV('data/Mappings.csv')\n",
    "    df = construct_input_vector(mappingloader)\n",
    "\n",
    "    return split_train_val_test(df)\n",
    "\n",
    "pre_process()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5414f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n",
      "Epoch: 0   time:  0.017   train_loss: 110.64489348542367   val_loss: 9.60288786522363\n",
      "Epoch: 1   time:  0.016   train_loss: 33.46845947409986   val_loss: 4.5264370799247295\n",
      "Epoch: 2   time:  0.027   train_loss: 27.029836680949447   val_loss: 4.515132221969664\n",
      "Epoch: 3   time:  0.016   train_loss: 26.26217221797925   val_loss: 4.84353076904525\n",
      "Epoch: 4   time:  0.018   train_loss: 20.890979660346186   val_loss: 4.415237774665875\n",
      "Epoch: 5   time:  0.015   train_loss: 20.213639464122885   val_loss: 4.424937820572406\n",
      "Epoch: 6   time:  0.01   train_loss: 21.332874064703475   val_loss: 5.800841423859511\n",
      "Epoch: 7   time:  0.017   train_loss: 22.709489624943593   val_loss: 4.41423223250965\n",
      "Epoch: 8   time:  0.016   train_loss: 24.120329493221313   val_loss: 5.425352388575004\n",
      "Epoch: 9   time:  0.016   train_loss: 25.334143377354177   val_loss: 4.411497943882785\n",
      "Epoch: 10   time:  0.022   train_loss: 21.45140794573699   val_loss: 4.513031514767235\n",
      "Epoch: 11   time:  0.016   train_loss: 23.120680191710186   val_loss: 4.398994248185312\n",
      "Epoch: 12   time:  0.018   train_loss: 21.95179713021353   val_loss: 4.641913335002732\n",
      "Epoch: 13   time:  0.015   train_loss: 22.43852371279067   val_loss: 4.3975988720241235\n",
      "Epoch: 14   time:  0.02   train_loss: 21.353424649431727   val_loss: 4.6587957515314375\n",
      "Epoch: 15   time:  0.017   train_loss: 23.116375956233856   val_loss: 4.3889362120308295\n",
      "Epoch: 16   time:  0.019   train_loss: 26.301690963558656   val_loss: 4.6152931357520925\n",
      "Epoch: 17   time:  0.022   train_loss: 24.566131052437157   val_loss: 4.452313116462881\n",
      "Epoch: 18   time:  0.02   train_loss: 20.752523212873204   val_loss: 4.467688059305192\n",
      "Epoch: 19   time:  0.018   train_loss: 24.672547207794754   val_loss: 4.420877755552441\n",
      "Epoch: 20   time:  0.021   train_loss: 20.753033440922433   val_loss: 4.454994421307062\n",
      "Epoch: 21   time:  0.023   train_loss: 21.22721512305954   val_loss: 4.696946633778242\n",
      "Epoch: 22   time:  0.075   train_loss: 25.516250618501317   val_loss: 4.4867572339305735\n",
      "Epoch: 23   time:  0.038   train_loss: 21.522239331664874   val_loss: 5.570264155851468\n",
      "Epoch: 24   time:  0.019   train_loss: 22.37567909841927   val_loss: 6.307809846946896\n",
      "Epoch: 25   time:  0.022   train_loss: 24.969047266075286   val_loss: 4.509511125029064\n",
      "Epoch: 26   time:  0.02   train_loss: 19.568870550742165   val_loss: 4.350224227488089\n",
      "Epoch: 27   time:  0.013   train_loss: 20.476881636663343   val_loss: 4.3583303034706935\n",
      "Epoch: 28   time:  0.016   train_loss: 21.33446531919216   val_loss: 4.362164316807849\n",
      "Epoch: 29   time:  0.016   train_loss: 18.33349531008482   val_loss: 4.344639049555671\n",
      "Epoch: 30   time:  0.022   train_loss: 22.360231486012033   val_loss: 4.336119179576093\n",
      "Epoch: 31   time:  0.024   train_loss: 19.658494396913948   val_loss: 4.8261908333490195\n",
      "Epoch: 32   time:  0.006   train_loss: 22.304655304237976   val_loss: 4.341138276998565\n",
      "Epoch: 33   time:  0.011   train_loss: 27.55649522912248   val_loss: 7.32741833875384\n",
      "Epoch: 34   time:  0.025   train_loss: 24.473591372909848   val_loss: 4.42723224151282\n",
      "Epoch: 35   time:  0.016   train_loss: 19.044237777367677   val_loss: 4.6995276225258245\n",
      "Epoch: 36   time:  0.027   train_loss: 19.63371074730518   val_loss: 4.3926151026814235\n",
      "Epoch: 37   time:  0.02   train_loss: 20.751538696685113   val_loss: 4.323080040987416\n",
      "Epoch: 38   time:  0.013   train_loss: 20.81929243228083   val_loss: 4.582103449331709\n",
      "Epoch: 39   time:  0.026   train_loss: 22.692540918474897   val_loss: 4.399921503055177\n",
      "Epoch: 40   time:  0.02   train_loss: 20.299899238326976   val_loss: 4.3216689140950155\n",
      "Epoch: 41   time:  0.019   train_loss: 21.98156778284351   val_loss: 4.4411811971898105\n",
      "Epoch: 42   time:  0.019   train_loss: 20.675594456658754   val_loss: 6.252098905755663\n",
      "Epoch: 43   time:  0.018   train_loss: 22.119337017202476   val_loss: 4.528904052656853\n",
      "Epoch: 44   time:  0.02   train_loss: 25.67538807213453   val_loss: 6.931716708881776\n",
      "Epoch: 45   time:  0.021   train_loss: 21.29127880377105   val_loss: 4.311467478160597\n",
      "Epoch: 46   time:  0.021   train_loss: 19.208422960079634   val_loss: 4.332531953905713\n",
      "Epoch: 47   time:  0.02   train_loss: 20.933297927176724   val_loss: 4.722576234786826\n",
      "Epoch: 48   time:  0.02   train_loss: 19.96892010048477   val_loss: 4.355474382176465\n",
      "Epoch: 49   time:  0.015   train_loss: 20.555889745928166   val_loss: 4.384857647887853\n",
      "Epoch: 50   time:  0.019   train_loss: 19.694202028396607   val_loss: 4.4459143631197575\n",
      "Epoch: 51   time:  0.016   train_loss: 19.321406556531596   val_loss: 4.557547871495996\n",
      "Epoch: 52   time:  0.018   train_loss: 19.98403497418578   val_loss: 4.417373984683725\n",
      "Epoch: 53   time:  0.016   train_loss: 21.40566894677981   val_loss: 4.291819272953783\n",
      "Epoch: 54   time:  0.021   train_loss: 20.839786615494468   val_loss: 5.251257540850834\n",
      "Epoch: 55   time:  0.011   train_loss: 19.61122741937171   val_loss: 6.259663205480808\n",
      "Epoch: 56   time:  0.025   train_loss: 19.69420744121536   val_loss: 4.726859752998034\n",
      "Epoch: 57   time:  0.021   train_loss: 18.01073393261995   val_loss: 4.9305076496604\n",
      "Epoch: 58   time:  0.017   train_loss: 23.700005089031105   val_loss: 4.815690695520956\n",
      "Epoch: 59   time:  0.025   train_loss: 19.87181472458427   val_loss: 4.3943964758138\n",
      "Epoch: 60   time:  0.016   train_loss: 20.097037666666147   val_loss: 4.372128029854893\n",
      "Epoch: 61   time:  0.017   train_loss: 20.161256326363688   val_loss: 4.922633770340099\n",
      "Epoch: 62   time:  0.019   train_loss: 22.11783606062417   val_loss: 4.465231123658406\n",
      "Epoch: 63   time:  0.021   train_loss: 19.19111920884374   val_loss: 4.512645845851565\n",
      "Epoch: 64   time:  0.019   train_loss: 17.15497470748049   val_loss: 4.542379100563039\n",
      "Epoch: 65   time:  0.01   train_loss: 19.128822844398744   val_loss: 4.354468654905518\n",
      "Epoch: 66   time:  0.027   train_loss: 21.09097310630429   val_loss: 5.306303717941619\n",
      "Epoch: 67   time:  0.017   train_loss: 23.9274068185301   val_loss: 4.5431340524632535\n",
      "Epoch: 68   time:  0.017   train_loss: 21.991168909037675   val_loss: 4.5288281523289715\n",
      "Epoch: 69   time:  0.01   train_loss: 19.639435605510222   val_loss: 4.531880201174923\n",
      "Epoch: 70   time:  0.016   train_loss: 18.425889610249065   val_loss: 4.455537554782858\n",
      "Epoch: 71   time:  0.01   train_loss: 18.46155779176339   val_loss: 5.103857169994424\n",
      "Epoch: 72   time:  0.027   train_loss: 24.681185887840456   val_loss: 8.578711157177615\n",
      "Epoch: 73   time:  0.014   train_loss: 21.92937620787249   val_loss: 5.636036650442687\n",
      "Epoch: 74   time:  0.017   train_loss: 18.701378791469235   val_loss: 4.311786059245881\n",
      "Epoch: 75   time:  0.017   train_loss: 18.86328990368478   val_loss: 4.321761571424327\n",
      "Epoch: 76   time:  0.021   train_loss: 20.151948151949586   val_loss: 5.171462036224211\n",
      "Epoch: 77   time:  0.017   train_loss: 15.89944467518068   val_loss: 6.873564113971984\n",
      "Epoch: 78   time:  0.016   train_loss: 20.607692903944812   val_loss: 4.445210626105681\n",
      "Epoch: 79   time:  0.019   train_loss: 20.243117203243532   val_loss: 4.875235927761602\n",
      "Epoch: 80   time:  0.018   train_loss: 24.104686376324608   val_loss: 4.509022016266682\n",
      "Epoch: 81   time:  0.026   train_loss: 22.30991314800964   val_loss: 7.476684244341226\n",
      "Epoch: 82   time:  0.015   train_loss: 26.211170855115768   val_loss: 4.478163231128394\n",
      "Epoch: 83   time:  0.022   train_loss: 20.397221842209852   val_loss: 4.6935061530126605\n",
      "Epoch: 84   time:  0.016   train_loss: 18.38875898715009   val_loss: 4.7698122762249575\n",
      "Epoch: 85   time:  0.006   train_loss: 18.24140169574048   val_loss: 4.304409974347521\n",
      "Epoch: 86   time:  0.028   train_loss: 19.982588506706747   val_loss: 4.370348063060352\n",
      "Epoch: 87   time:  0.015   train_loss: 20.162260858186904   val_loss: 5.040646113540086\n",
      "Epoch: 88   time:  0.009   train_loss: 21.15592990373572   val_loss: 7.473133363659394\n",
      "Epoch: 89   time:  0.024   train_loss: 23.336556716150877   val_loss: 4.464871989573747\n",
      "Epoch: 90   time:  0.017   train_loss: 17.165847841867045   val_loss: 4.286838623645891\n",
      "Epoch: 91   time:  0.019   train_loss: 17.727556641927276   val_loss: 4.586270181397521\n",
      "Epoch: 92   time:  0.015   train_loss: 15.80017198939081   val_loss: 4.457424610718369\n",
      "Epoch: 93   time:  0.016   train_loss: 20.605701934750517   val_loss: 4.683407090714119\n",
      "Epoch: 94   time:  0.016   train_loss: 17.535669401100076   val_loss: 7.4656798251485395\n",
      "Epoch: 95   time:  0.016   train_loss: 21.660832912956156   val_loss: 4.803245829634948\n",
      "Epoch: 96   time:  0.016   train_loss: 18.575390610880827   val_loss: 4.536268743390409\n",
      "Epoch: 97   time:  0.017   train_loss: 20.35091622051527   val_loss: 4.639279758385481\n",
      "Epoch: 98   time:  0.017   train_loss: 19.63597997988778   val_loss: 4.202031432218306\n",
      "Epoch: 99   time:  0.017   train_loss: 18.85741797041206   val_loss: 5.246861070428477\n"
     ]
    }
   ],
   "source": [
    "# 5. Model Definitions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sig(x):\n",
    "    return 1/(1+np.exp(-np.clip(x, -500, 500)))  # Clip to prevent overflow\n",
    "\n",
    "def sig_derivative(x):\n",
    "    s = sig(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_derivative(x):\n",
    "    return np.ones_like(x)\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "architecture={\n",
    "    'A':[110,256,1],\n",
    "    'B':[110,128,128,1]\n",
    "}\n",
    "\n",
    "class NeuralNetwrokArgs:\n",
    "\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        self.layer_dims = architecture[self.name]\n",
    "        self.activation:Callable[[np.ndarray],np.ndarray]=relu\n",
    "        self.activation_derivative:Callable[[np.ndarray],np.ndarray] = relu_derivative\n",
    "        self.lr=0.001\n",
    "        self.output_activation:Callable[[np.ndarray],np.ndarray] = linear\n",
    "        self.output_activation_derivative = linear_derivative\n",
    "        self.epochs=100\n",
    "        self.batch_size=16\n",
    "\n",
    "class Best:\n",
    "    def __init__(self,biases,weights):\n",
    "        self.biases= deepcopy(biases)\n",
    "        self.weights= deepcopy(weights)\n",
    "        self.lowest_val_loss=float(\"inf\")\n",
    "\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, neuralArgs:NeuralNetwrokArgs):\n",
    "\n",
    "        self.name = neuralArgs.name\n",
    "        self.layer_dims= neuralArgs.layer_dims\n",
    "        self.weights = []\n",
    "        self.biases= []\n",
    "        self.activation = neuralArgs.activation\n",
    "        self.activation_derivative = neuralArgs.activation_derivative\n",
    "        self.output_activation = neuralArgs.output_activation\n",
    "        self.output_activation_derivative = neuralArgs.output_activation_derivative\n",
    "        self.lr= neuralArgs.lr\n",
    "        self.batch_size = neuralArgs.batch_size\n",
    "        self.epochs = neuralArgs.epochs\n",
    "        for i in range(1,len(self.layer_dims)):\n",
    "            w = np.random.normal(loc=0.0, scale=0.01, size=(self.layer_dims[i], self.layer_dims[i-1]))\n",
    "            self.weights.append(w)\n",
    "            b= np.zeros((self.layer_dims[i],1))\n",
    "            self.biases.append(b)\n",
    "        self.best= Best(self.biases,self.weights)\n",
    "        \n",
    "        \n",
    "    def snapshot(self,val_loss):\n",
    "        if val_loss< self.best.lowest_val_loss:\n",
    "            self.best.weights = deepcopy(self.weights)\n",
    "            self.best.biases = deepcopy(self.biases)\n",
    "            self.best.lowest_val_loss = val_loss\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        cache_a=[x.T]\n",
    "        cache_z=[]\n",
    "        for i in range(len(self.layer_dims)-1):\n",
    "            z=self.weights[i].dot(cache_a[i])+self.biases[i]\n",
    "            if i == len(self.layer_dims) - 2:  # Output layer\n",
    "                a = self.output_activation(z)\n",
    "            else:\n",
    "                a = self.activation(z)\n",
    "            cache_a.append(a)\n",
    "            cache_z.append(z)\n",
    "        return cache_a,cache_z\n",
    "\n",
    "    def backward(self,y_true,cache_a,cache_z):\n",
    "        grads={}\n",
    "        deltas=[]\n",
    "        y_true = np.array(y_true)\n",
    "        if y_true.ndim == 1:\n",
    "            y_true = y_true.reshape(1, -1) \n",
    "        #Output layer\n",
    "        delta_output = 2* (cache_a[-1]-y_true) * self.output_activation_derivative(cache_z[-1])\n",
    "        deltas.append(delta_output)\n",
    "\n",
    "        #hidden layers\n",
    "        current_delta = delta_output\n",
    "\n",
    "        for l in range(len(self.layer_dims)-2,0,-1):\n",
    "            delta_hidden = self.weights[l].T.dot(current_delta) * self.activation_derivative(cache_z[l-1])\n",
    "            deltas.insert(0, delta_hidden)\n",
    "            current_delta = delta_hidden\n",
    "\n",
    "        for l in range(len(self.weights)):\n",
    "            # ∂L/∂W^(l) = δ^(l) (a^(l-1))^T\n",
    "            dW = deltas[l].dot(cache_a[l].T)\n",
    "            \n",
    "            # ∂L/∂b^(l) = δ^(l)\n",
    "            db = np.sum(deltas[l], axis=1, keepdims=True)\n",
    "            \n",
    "            grads[f'dW{l+1}'] = dW\n",
    "            grads[f'db{l+1}'] = db\n",
    "        \n",
    "        return grads\n",
    "\n",
    "\n",
    "\n",
    "    def update_params(self,grads):\n",
    "        for l in range(len(self.weights)):\n",
    "            grads[f'dW{l+1}'] = np.clip(grads[f'dW{l+1}'], -1, 1)\n",
    "            grads[f'db{l+1}'] = np.clip(grads[f'db{l+1}'], -1, 1)\n",
    "            self.weights[l] -= self.lr * grads[f'dW{l+1}']\n",
    "            self.biases[l] -= self.lr * grads[f'db{l+1}']\n",
    "    \n",
    "\n",
    "\n",
    "    # 6. Training Loop\n",
    "    def train(self, trainData:DataFrame,valData:DataFrame,useBest=True,silent=False):\n",
    "        if useBest:\n",
    "            self.weights = self.best.weights\n",
    "            self.biases = self.best.biases\n",
    "\n",
    "        for e in range(self.epochs):\n",
    "            start = time.time()\n",
    "            trainData = trainData.sample(frac=1).reset_index(drop=True)\n",
    "            batches = create_batches(trainData,self.batch_size)\n",
    "            train_loss =0\n",
    "            for batch in batches:\n",
    "                x_train,y_train =split_x_y(batch)\n",
    "                cache_a,cache_z = self.forward(x_train)\n",
    "                train_loss+=mse(y_train,cache_a[-1])\n",
    "                # print()\n",
    "                grads = self.backward(y_train,cache_a,cache_z)\n",
    "                self.update_params(grads)\n",
    "            end = time.time()\n",
    "            val_pred, val_loss = self.evaluate(valData)\n",
    "            self.snapshot(val_loss)\n",
    "            if not silent:\n",
    "                print(f\"Epoch: {e}   time:  {round(end-start,3)}   train_loss: {train_loss}   val_loss: {val_loss}\")\n",
    "        \n",
    "\n",
    "\n",
    "    def evaluate(self,evalData):\n",
    "        x,y_true =split_x_y(evalData)\n",
    "        cache_a,cache_z = self.forward(x)\n",
    "        return cache_a[-1],mse(y_true,cache_a[-1])\n",
    "\n",
    "    def predict(self,predData):\n",
    "        return self.forward(predData)[0][-1]\n",
    "\n",
    "\n",
    "\n",
    "trainData,valData,testData = pre_process()\n",
    "args = NeuralNetwrokArgs('A')\n",
    "nn = NeuralNetwork(args)\n",
    "print(nn.best.lowest_val_loss)\n",
    "nn.epochs=100\n",
    "nn.train(trainData,valData)\n",
    "test_pred,test_loss = nn.evaluate(testData)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 7. Evaluation & Plots\n",
    "#- Generate the eight required figures\n",
    "#- Save each via plt.savefig()\n",
    "# 8. Save & Export\n",
    "#- Download figures\n",
    "#- Optionally, pickle model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0a5dbc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.epochs= 200\n",
    "# nn.train(trainData,valData,useBest=True)\n",
    "# print(nn.best.lowest_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "de29bf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0/108\n",
      "Iteration: 1/108\n",
      "Iteration: 2/108\n",
      "Iteration: 3/108\n",
      "Iteration: 4/108\n",
      "Iteration: 5/108\n",
      "Iteration: 6/108\n",
      "Iteration: 7/108\n",
      "Iteration: 8/108\n",
      "Iteration: 9/108\n",
      "Iteration: 10/108\n",
      "Iteration: 11/108\n",
      "Iteration: 12/108\n",
      "Iteration: 13/108\n",
      "Iteration: 14/108\n",
      "Iteration: 15/108\n",
      "Iteration: 16/108\n",
      "Iteration: 17/108\n",
      "Iteration: 18/108\n",
      "Iteration: 19/108\n",
      "Iteration: 20/108\n",
      "Iteration: 21/108\n",
      "Iteration: 22/108\n",
      "Iteration: 23/108\n",
      "Iteration: 24/108\n",
      "Iteration: 25/108\n",
      "Iteration: 26/108\n",
      "Iteration: 27/108\n",
      "Iteration: 28/108\n",
      "Iteration: 29/108\n",
      "Iteration: 30/108\n",
      "Iteration: 31/108\n",
      "Iteration: 32/108\n",
      "Iteration: 33/108\n",
      "Iteration: 34/108\n",
      "Iteration: 35/108\n",
      "Iteration: 36/108\n",
      "Iteration: 37/108\n",
      "Iteration: 38/108\n",
      "Iteration: 39/108\n",
      "Iteration: 40/108\n",
      "Iteration: 41/108\n",
      "Iteration: 42/108\n",
      "Iteration: 43/108\n",
      "Iteration: 44/108\n",
      "Iteration: 45/108\n",
      "Iteration: 46/108\n",
      "Iteration: 47/108\n",
      "Iteration: 48/108\n",
      "Iteration: 49/108\n",
      "Iteration: 50/108\n",
      "Iteration: 51/108\n",
      "Iteration: 52/108\n",
      "Iteration: 53/108\n",
      "Iteration: 54/108\n",
      "Iteration: 55/108\n",
      "Iteration: 56/108\n",
      "Iteration: 57/108\n",
      "Iteration: 58/108\n",
      "Iteration: 59/108\n",
      "Iteration: 60/108\n",
      "Iteration: 61/108\n",
      "Iteration: 62/108\n",
      "Iteration: 63/108\n",
      "Iteration: 64/108\n",
      "Iteration: 65/108\n",
      "Iteration: 66/108\n",
      "Iteration: 67/108\n",
      "Iteration: 68/108\n",
      "Iteration: 69/108\n",
      "Iteration: 70/108\n",
      "Iteration: 71/108\n",
      "Iteration: 72/108\n",
      "Iteration: 73/108\n",
      "Iteration: 74/108\n",
      "Iteration: 75/108\n",
      "Iteration: 76/108\n",
      "Iteration: 77/108\n",
      "Iteration: 78/108\n",
      "Iteration: 79/108\n",
      "Iteration: 80/108\n",
      "Iteration: 81/108\n",
      "Iteration: 82/108\n",
      "Iteration: 83/108\n",
      "Iteration: 84/108\n",
      "Iteration: 85/108\n",
      "Iteration: 86/108\n",
      "Iteration: 87/108\n",
      "Iteration: 88/108\n",
      "Iteration: 89/108\n",
      "Iteration: 90/108\n",
      "Iteration: 91/108\n",
      "Iteration: 92/108\n",
      "Iteration: 93/108\n",
      "Iteration: 94/108\n",
      "Iteration: 95/108\n",
      "Iteration: 96/108\n",
      "Iteration: 97/108\n",
      "Iteration: 98/108\n",
      "Iteration: 99/108\n",
      "Iteration: 100/108\n",
      "Iteration: 101/108\n",
      "Iteration: 102/108\n",
      "Iteration: 103/108\n",
      "Iteration: 104/108\n",
      "Iteration: 105/108\n",
      "Iteration: 106/108\n",
      "Iteration: 107/108\n",
      "#1: Model=A, LR=0.01, Batch=8, Epochs=200, Act=sig, Val Loss=3.6814\n",
      "#2: Model=A, LR=0.001, Batch=8, Epochs=200, Act=relu, Val Loss=3.7902\n",
      "#3: Model=B, LR=0.01, Batch=8, Epochs=200, Act=sig, Val Loss=3.8909\n",
      "#4: Model=A, LR=0.01, Batch=8, Epochs=150, Act=sig, Val Loss=3.9633\n",
      "#5: Model=A, LR=0.01, Batch=16, Epochs=200, Act=sig, Val Loss=3.9802\n",
      "#6: Model=A, LR=0.001, Batch=16, Epochs=200, Act=relu, Val Loss=4.0502\n",
      "#7: Model=B, LR=0.001, Batch=8, Epochs=200, Act=relu, Val Loss=4.0798\n",
      "#8: Model=B, LR=0.001, Batch=16, Epochs=150, Act=relu, Val Loss=4.0928\n",
      "#9: Model=B, LR=0.001, Batch=16, Epochs=200, Act=relu, Val Loss=4.0941\n",
      "#10: Model=A, LR=0.001, Batch=8, Epochs=200, Act=sig, Val Loss=4.0971\n",
      "#11: Model=A, LR=0.01, Batch=16, Epochs=100, Act=sig, Val Loss=4.0975\n",
      "#12: Model=A, LR=0.01, Batch=32, Epochs=200, Act=sig, Val Loss=4.0976\n",
      "#13: Model=A, LR=0.01, Batch=8, Epochs=100, Act=sig, Val Loss=4.1031\n",
      "#14: Model=B, LR=0.001, Batch=8, Epochs=150, Act=relu, Val Loss=4.1062\n",
      "#15: Model=B, LR=0.01, Batch=16, Epochs=150, Act=sig, Val Loss=4.1161\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class HyperArgs:\n",
    "    names=['A','B']\n",
    "    lr=[0.01, 0.001, 0.0001]\n",
    "    batch_size=[8,16,32]\n",
    "    epochs=[100,150,200]\n",
    "    activations=[[sig,sig_derivative],[relu,relu_derivative]]\n",
    "    def totalRuns(self):\n",
    "        return len(self.names) * len(self.lr) * len(self.batch_size) * len(self.epochs) * len(self.activations)\n",
    "\n",
    "\n",
    "\n",
    "def hyper_grid_search(gridArgs:HyperArgs):\n",
    "    trainData,valData,testData = pre_process()\n",
    "    results=[]\n",
    "    total_runs = gridArgs.totalRuns()\n",
    "    run=0\n",
    "    for name in gridArgs.names:\n",
    "        for lr in gridArgs.lr:\n",
    "            for batch in gridArgs.batch_size:\n",
    "                for epoch in gridArgs.epochs:\n",
    "                    for activation in gridArgs.activations:\n",
    "                        print(f\"Iteration: {run}/{total_runs}\")\n",
    "                        run+=1\n",
    "                        args = NeuralNetwrokArgs(name)\n",
    "                        args.lr = lr\n",
    "                        args.batch_size = batch\n",
    "                        args.epochs = epoch\n",
    "                        args.activation = activation[0]\n",
    "                        args.activation_derivative = activation[1]\n",
    "                        args.layer_dims = architecture[name]\n",
    "                        nn= NeuralNetwork(args)\n",
    "                        nn.train(trainData,valData,silent=True)\n",
    "                        results.append({\n",
    "                            \"model_name\": name,\n",
    "                            \"lr\": lr,\n",
    "                            \"batch_size\": batch,\n",
    "                            \"epochs\": epoch,\n",
    "                            \"activation\": activation[0].__name__,\n",
    "                            \"val_loss\": nn.best.lowest_val_loss,\n",
    "                            \"nn\": nn  \n",
    "                        })\n",
    "    results.sort(key= lambda x: x['val_loss'])\n",
    "    top_15 = results[:15]\n",
    "    for i, res in enumerate(top_15, 1):\n",
    "        print(f\"#{i}: Model={res['model_name']}, LR={res['lr']}, Batch={res['batch_size']}, \"\n",
    "            f\"Epochs={res['epochs']}, Act={res['activation']}, Val Loss={res['val_loss']:.4f}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "results = hyper_grid_search(HyperArgs())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5cfa2142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1: Model=A, LR=0.01, Batch=8, Epochs=200, Act=sig, Val Loss=3.6814\n",
      "#2: Model=A, LR=0.001, Batch=8, Epochs=200, Act=relu, Val Loss=3.7902\n",
      "#3: Model=B, LR=0.01, Batch=8, Epochs=200, Act=sig, Val Loss=3.8909\n",
      "#4: Model=A, LR=0.01, Batch=8, Epochs=150, Act=sig, Val Loss=3.9633\n",
      "#5: Model=A, LR=0.01, Batch=16, Epochs=200, Act=sig, Val Loss=3.9802\n",
      "#6: Model=A, LR=0.001, Batch=16, Epochs=200, Act=relu, Val Loss=4.0502\n",
      "#7: Model=B, LR=0.001, Batch=8, Epochs=200, Act=relu, Val Loss=4.0798\n",
      "#8: Model=B, LR=0.001, Batch=16, Epochs=150, Act=relu, Val Loss=4.0928\n",
      "#9: Model=B, LR=0.001, Batch=16, Epochs=200, Act=relu, Val Loss=4.0941\n",
      "#10: Model=A, LR=0.001, Batch=8, Epochs=200, Act=sig, Val Loss=4.0971\n",
      "#11: Model=A, LR=0.01, Batch=16, Epochs=100, Act=sig, Val Loss=4.0975\n",
      "#12: Model=A, LR=0.01, Batch=32, Epochs=200, Act=sig, Val Loss=4.0976\n",
      "#13: Model=A, LR=0.01, Batch=8, Epochs=100, Act=sig, Val Loss=4.1031\n",
      "#14: Model=B, LR=0.001, Batch=8, Epochs=150, Act=relu, Val Loss=4.1062\n",
      "#15: Model=B, LR=0.01, Batch=16, Epochs=150, Act=sig, Val Loss=4.1161\n",
      "#16: Model=A, LR=0.001, Batch=16, Epochs=200, Act=sig, Val Loss=4.1181\n",
      "#17: Model=A, LR=0.001, Batch=8, Epochs=150, Act=relu, Val Loss=4.1216\n",
      "#18: Model=B, LR=0.01, Batch=16, Epochs=200, Act=sig, Val Loss=4.1244\n",
      "#19: Model=A, LR=0.01, Batch=16, Epochs=150, Act=sig, Val Loss=4.1356\n",
      "#20: Model=B, LR=0.01, Batch=16, Epochs=100, Act=sig, Val Loss=4.1380\n",
      "#21: Model=A, LR=0.001, Batch=32, Epochs=200, Act=sig, Val Loss=4.1381\n",
      "#22: Model=A, LR=0.001, Batch=32, Epochs=150, Act=sig, Val Loss=4.1388\n",
      "#23: Model=B, LR=0.01, Batch=32, Epochs=200, Act=sig, Val Loss=4.1392\n",
      "#24: Model=A, LR=0.001, Batch=8, Epochs=150, Act=sig, Val Loss=4.1408\n",
      "#25: Model=A, LR=0.001, Batch=16, Epochs=150, Act=sig, Val Loss=4.1421\n",
      "#26: Model=A, LR=0.001, Batch=32, Epochs=100, Act=sig, Val Loss=4.1497\n",
      "#27: Model=A, LR=0.001, Batch=32, Epochs=150, Act=relu, Val Loss=4.1555\n",
      "#28: Model=A, LR=0.001, Batch=8, Epochs=100, Act=sig, Val Loss=4.1653\n",
      "#29: Model=B, LR=0.001, Batch=8, Epochs=200, Act=sig, Val Loss=4.1684\n",
      "#30: Model=A, LR=0.001, Batch=16, Epochs=100, Act=sig, Val Loss=4.1705\n",
      "#31: Model=B, LR=0.01, Batch=8, Epochs=100, Act=sig, Val Loss=4.1720\n",
      "#32: Model=B, LR=0.01, Batch=32, Epochs=100, Act=sig, Val Loss=4.1721\n",
      "#33: Model=A, LR=0.01, Batch=32, Epochs=150, Act=sig, Val Loss=4.1766\n",
      "#34: Model=B, LR=0.01, Batch=8, Epochs=150, Act=sig, Val Loss=4.1775\n",
      "#35: Model=A, LR=0.01, Batch=32, Epochs=100, Act=sig, Val Loss=4.1798\n",
      "#36: Model=B, LR=0.01, Batch=32, Epochs=150, Act=sig, Val Loss=4.1941\n",
      "#37: Model=B, LR=0.001, Batch=32, Epochs=150, Act=relu, Val Loss=4.1941\n",
      "#38: Model=A, LR=0.01, Batch=8, Epochs=200, Act=relu, Val Loss=4.1995\n",
      "#39: Model=B, LR=0.001, Batch=8, Epochs=150, Act=sig, Val Loss=4.2071\n",
      "#40: Model=B, LR=0.001, Batch=16, Epochs=200, Act=sig, Val Loss=4.2085\n",
      "#41: Model=B, LR=0.001, Batch=32, Epochs=200, Act=sig, Val Loss=4.2123\n",
      "#42: Model=B, LR=0.001, Batch=16, Epochs=150, Act=sig, Val Loss=4.2126\n",
      "#43: Model=B, LR=0.001, Batch=32, Epochs=150, Act=sig, Val Loss=4.2139\n",
      "#44: Model=B, LR=0.001, Batch=16, Epochs=100, Act=sig, Val Loss=4.2140\n",
      "#45: Model=B, LR=0.001, Batch=32, Epochs=100, Act=sig, Val Loss=4.2144\n",
      "#46: Model=B, LR=0.0001, Batch=32, Epochs=200, Act=sig, Val Loss=4.2149\n",
      "#47: Model=B, LR=0.001, Batch=8, Epochs=100, Act=sig, Val Loss=4.2162\n",
      "#48: Model=B, LR=0.0001, Batch=16, Epochs=200, Act=sig, Val Loss=4.2165\n",
      "#49: Model=B, LR=0.0001, Batch=16, Epochs=150, Act=sig, Val Loss=4.2167\n",
      "#50: Model=B, LR=0.0001, Batch=16, Epochs=100, Act=sig, Val Loss=4.2172\n",
      "#51: Model=B, LR=0.0001, Batch=8, Epochs=150, Act=sig, Val Loss=4.2182\n",
      "#52: Model=B, LR=0.0001, Batch=32, Epochs=150, Act=sig, Val Loss=4.2190\n",
      "#53: Model=B, LR=0.0001, Batch=8, Epochs=100, Act=sig, Val Loss=4.2193\n",
      "#54: Model=B, LR=0.0001, Batch=8, Epochs=200, Act=sig, Val Loss=4.2198\n",
      "#55: Model=A, LR=0.0001, Batch=16, Epochs=150, Act=sig, Val Loss=4.2246\n",
      "#56: Model=A, LR=0.0001, Batch=32, Epochs=150, Act=sig, Val Loss=4.2257\n",
      "#57: Model=A, LR=0.0001, Batch=32, Epochs=200, Act=sig, Val Loss=4.2276\n",
      "#58: Model=A, LR=0.0001, Batch=32, Epochs=100, Act=sig, Val Loss=4.2282\n",
      "#59: Model=A, LR=0.0001, Batch=16, Epochs=200, Act=sig, Val Loss=4.2289\n",
      "#60: Model=A, LR=0.0001, Batch=16, Epochs=100, Act=sig, Val Loss=4.2298\n",
      "#61: Model=A, LR=0.0001, Batch=8, Epochs=150, Act=sig, Val Loss=4.2321\n",
      "#62: Model=A, LR=0.0001, Batch=8, Epochs=200, Act=sig, Val Loss=4.2332\n",
      "#63: Model=A, LR=0.0001, Batch=8, Epochs=100, Act=sig, Val Loss=4.2335\n",
      "#64: Model=A, LR=0.001, Batch=32, Epochs=200, Act=relu, Val Loss=4.2386\n",
      "#65: Model=B, LR=0.001, Batch=8, Epochs=100, Act=relu, Val Loss=4.2496\n",
      "#66: Model=B, LR=0.001, Batch=16, Epochs=100, Act=relu, Val Loss=4.2768\n",
      "#67: Model=A, LR=0.001, Batch=32, Epochs=100, Act=relu, Val Loss=4.2932\n",
      "#68: Model=B, LR=0.01, Batch=16, Epochs=150, Act=relu, Val Loss=4.3004\n",
      "#69: Model=A, LR=0.001, Batch=8, Epochs=100, Act=relu, Val Loss=4.3007\n",
      "#70: Model=A, LR=0.01, Batch=8, Epochs=150, Act=relu, Val Loss=4.3295\n",
      "#71: Model=A, LR=0.01, Batch=32, Epochs=200, Act=relu, Val Loss=4.3321\n",
      "#72: Model=A, LR=0.001, Batch=16, Epochs=150, Act=relu, Val Loss=4.3330\n",
      "#73: Model=B, LR=0.001, Batch=32, Epochs=100, Act=relu, Val Loss=4.3605\n",
      "#74: Model=A, LR=0.0001, Batch=32, Epochs=200, Act=relu, Val Loss=4.3677\n",
      "#75: Model=A, LR=0.001, Batch=16, Epochs=100, Act=relu, Val Loss=4.3731\n",
      "#76: Model=A, LR=0.0001, Batch=8, Epochs=200, Act=relu, Val Loss=4.3758\n",
      "#77: Model=B, LR=0.001, Batch=32, Epochs=200, Act=relu, Val Loss=4.3812\n",
      "#78: Model=A, LR=0.0001, Batch=16, Epochs=200, Act=relu, Val Loss=4.3966\n",
      "#79: Model=A, LR=0.0001, Batch=16, Epochs=150, Act=relu, Val Loss=4.4004\n",
      "#80: Model=B, LR=0.0001, Batch=8, Epochs=200, Act=relu, Val Loss=4.4024\n",
      "#81: Model=B, LR=0.01, Batch=8, Epochs=100, Act=relu, Val Loss=4.4049\n",
      "#82: Model=B, LR=0.0001, Batch=16, Epochs=200, Act=relu, Val Loss=4.4060\n",
      "#83: Model=A, LR=0.01, Batch=32, Epochs=150, Act=relu, Val Loss=4.4079\n",
      "#84: Model=B, LR=0.0001, Batch=16, Epochs=150, Act=relu, Val Loss=4.4112\n",
      "#85: Model=B, LR=0.0001, Batch=8, Epochs=150, Act=relu, Val Loss=4.4143\n",
      "#86: Model=B, LR=0.0001, Batch=32, Epochs=150, Act=relu, Val Loss=4.4193\n",
      "#87: Model=A, LR=0.0001, Batch=16, Epochs=100, Act=relu, Val Loss=4.4221\n",
      "#88: Model=A, LR=0.0001, Batch=32, Epochs=100, Act=relu, Val Loss=4.4247\n",
      "#89: Model=A, LR=0.0001, Batch=32, Epochs=150, Act=relu, Val Loss=4.4253\n",
      "#90: Model=B, LR=0.0001, Batch=32, Epochs=200, Act=relu, Val Loss=4.4272\n",
      "#91: Model=B, LR=0.0001, Batch=8, Epochs=100, Act=relu, Val Loss=4.4275\n",
      "#92: Model=A, LR=0.0001, Batch=8, Epochs=100, Act=relu, Val Loss=4.4304\n",
      "#93: Model=B, LR=0.01, Batch=32, Epochs=200, Act=relu, Val Loss=4.4317\n",
      "#94: Model=B, LR=0.0001, Batch=32, Epochs=100, Act=relu, Val Loss=4.4343\n",
      "#95: Model=A, LR=0.01, Batch=16, Epochs=200, Act=relu, Val Loss=4.4370\n",
      "#96: Model=A, LR=0.0001, Batch=8, Epochs=150, Act=relu, Val Loss=4.4382\n",
      "#97: Model=B, LR=0.0001, Batch=16, Epochs=100, Act=relu, Val Loss=4.4384\n",
      "#98: Model=B, LR=0.01, Batch=8, Epochs=150, Act=relu, Val Loss=4.4856\n",
      "#99: Model=A, LR=0.01, Batch=32, Epochs=100, Act=relu, Val Loss=4.4914\n",
      "#100: Model=A, LR=0.01, Batch=16, Epochs=100, Act=relu, Val Loss=4.5108\n",
      "#101: Model=B, LR=0.01, Batch=32, Epochs=150, Act=relu, Val Loss=4.5190\n",
      "#102: Model=B, LR=0.01, Batch=8, Epochs=200, Act=relu, Val Loss=4.5929\n",
      "#103: Model=B, LR=0.01, Batch=32, Epochs=100, Act=relu, Val Loss=4.6839\n",
      "#104: Model=A, LR=0.01, Batch=16, Epochs=150, Act=relu, Val Loss=5.0128\n",
      "#105: Model=A, LR=0.01, Batch=8, Epochs=100, Act=relu, Val Loss=5.3279\n",
      "#106: Model=B, LR=0.01, Batch=16, Epochs=100, Act=relu, Val Loss=6.4461\n",
      "#107: Model=B, LR=0.01, Batch=16, Epochs=200, Act=relu, Val Loss=6.5967\n",
      "#108: Model=B, LR=0.0001, Batch=32, Epochs=100, Act=sig, Val Loss=7.0618\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(results, 1):\n",
    "        print(f\"#{i}: Model={res['model_name']}, LR={res['lr']}, Batch={res['batch_size']}, \"\n",
    "            f\"Epochs={res['epochs']}, Act={res['activation']}, Val Loss={res['val_loss']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
