{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "543cf45a",
   "metadata": {},
   "source": [
    "# 1. Notebook Setup\n",
    "# Title, assignment info, and markdown overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "044f2c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "import csv\n",
    "from typing import Union\n",
    "from typing import Callable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "48957c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_file_path = \"data/Mappings.csv\"\n",
    "employees_file_path = \"data/Employee.csv\"\n",
    "tasks_file_path = \"data/Tasks.csv\"\n",
    "rd.seed(20)\n",
    "\n",
    "# Data Classes\n",
    "class Task:\n",
    "    time:int =0\n",
    "    difficulty:int =0\n",
    "    deadline:int = 0\n",
    "    required_skill:str = ''\n",
    "    def __str__(self):\n",
    "        return f\"T Time:{self.time} Difficulty:{self.difficulty} Deadline:{self.deadline} requiredSkill:{self.required_skill}\"\n",
    "\n",
    "class Employee:\n",
    "    available_hours:int = 0\n",
    "    skill_level:int = 4\n",
    "    skills:list[str]=['']\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"E AH:{self.available_hours} Skill-Level:{self.skill_level} Skills:{self.skills}\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7ea3c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O\n",
    "\"\"\"\n",
    "Task csv:\n",
    "\n",
    "'ID', 'Time (hrs)', 'Difficulty', 'Deadline (hrs)', 'Required Skill'\n",
    "\n",
    "Employee csv:\n",
    "'Employee ID', 'Available Hrs', 'Skill Level', 'Skills'\n",
    "\n",
    "\n",
    "Using an adjancey list instead of an adjacney matrix!\n",
    "\n",
    "Since the input vector is 10 mappings x 11 features. Including a unique assignment penalty is redundant since a task mapped to 2 employees will create (10+1) mappings which doesn't work for the network input layer!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    num_of_tasks =10\n",
    "    num_of_employees = 5\n",
    "    tasks:list[Task]=[]\n",
    "    employees:list[Employee]=[]\n",
    "    \n",
    "    def load_tasks(self,fileName=tasks_file_path):\n",
    "        tasks =[]\n",
    "        with open(fileName,'r') as csvfile:\n",
    "            taskReader = csv.reader(csvfile)\n",
    "            next(taskReader)\n",
    "            for taskArr in taskReader:\n",
    "                newT = Task()\n",
    "                newT.time=int(taskArr[1])\n",
    "                newT.difficulty= int(taskArr[2])\n",
    "                newT.deadline = int(taskArr[3])\n",
    "                newT.required_skill= taskArr[4]\n",
    "                tasks.append(newT)\n",
    "        self.tasks= tasks\n",
    "\n",
    "    def load_employees(self,fileName=employees_file_path):\n",
    "        employees=[]\n",
    "        with open(fileName,'r') as csvfile:\n",
    "            employeeReader = csv.reader(csvfile)\n",
    "            next(employeeReader)\n",
    "            for employeeArr in employeeReader:\n",
    "                newEmployee = Employee()\n",
    "                newEmployee.available_hours=int(employeeArr[1])\n",
    "                newEmployee.skill_level= int(employeeArr[2])\n",
    "                skills = employeeArr[3].split(',')\n",
    "                newEmployee.skills = skills\n",
    "                employees.append(newEmployee)\n",
    "        self.employees = employees\n",
    "    def loadAll(self):\n",
    "        self.load_employees()\n",
    "        self.load_tasks()\n",
    "    \n",
    "\n",
    "\n",
    "class MappingHandler:\n",
    "    def __init__(self,tasks:list[Task],employees:list[Employee],num_of_mappings=100):\n",
    "        self.tasks:list[Task]= tasks\n",
    "        self.employees:list[Employee] = employees\n",
    "        self.num_of_mappings = num_of_mappings\n",
    "        self.mappings = []\n",
    "        self.costs=[]\n",
    "\n",
    "\n",
    "    def __costFunction(self,Mapping:list[int]):\n",
    "        w=0.2\n",
    "        overload = 0\n",
    "        skill_mismatch = 0\n",
    "        difficulty_violation = 0\n",
    "        deadline_violation = 0\n",
    "        unique_assignment = 0\n",
    "        employee_task_adj_list:list[list[Task]]=[[] for _ in range(len(self.employees))]\n",
    "\n",
    "        for i in range(len(Mapping)):\n",
    "            taskId=i\n",
    "\n",
    "            task = self.tasks[taskId]\n",
    "            num_of_employees_assigned = 0\n",
    "            employeeId = Mapping[i]\n",
    "            num_of_employees_assigned+=1\n",
    "            employee = self.employees[employeeId]\n",
    "            employee_task_adj_list[employeeId].append(task)\n",
    "            # skill mismatch violation\n",
    "            if task.required_skill not in employee.skills:\n",
    "                skill_mismatch+=1\n",
    "            if task.difficulty> employee.skill_level:\n",
    "                difficulty_violation+= 1\n",
    "            unique_assignment+= max(0,num_of_employees_assigned-1)\n",
    "\n",
    "\n",
    "\n",
    "        for employeeId in range(len(employee_task_adj_list)):\n",
    "            employee = self.employees[employeeId]\n",
    "            sortedEmployeeTasks = employee_task_adj_list[employeeId]\n",
    "            sortedEmployeeTasks.sort(key= lambda x:x.time)\n",
    "            sumHours=0\n",
    "            finishTime=0\n",
    "            for t in sortedEmployeeTasks:\n",
    "                sumHours+=t.time\n",
    "                finishTime+= t.time\n",
    "                deadline_violation+= max(0,finishTime - t.deadline)\n",
    "            overload+= max(0,sumHours- employee.available_hours) \n",
    "        total_penalty = overload+skill_mismatch+unique_assignment+deadline_violation+ difficulty_violation\n",
    "        return round(total_penalty*w,3)\n",
    "    def __reset(self):\n",
    "        self.mappings=[]\n",
    "        self.costs=[]\n",
    "\n",
    "\n",
    "    def generateMappings(self):\n",
    "        unique = set()\n",
    "        self.__reset()\n",
    "        while len(unique) < self.num_of_mappings:\n",
    "            mapping:list[int] = [0 for _ in self.tasks]         # Create a mapping for each iteration. Using an adjlist format\n",
    "            # possible_assignments = [i for i in range(len(employees))]  #list of possible assignments\n",
    "            for taskId in range(len(self.tasks)):\n",
    "                rand_employee =rd.randint(0,len(self.employees)-1)\n",
    "                mapping[taskId]=rand_employee\n",
    "            string = str(mapping)\n",
    "            if string not in unique: # ensure no duplicates in data generation\n",
    "                unique.add(string)\n",
    "                self.mappings.append(mapping)\n",
    "                self.costs.append(self.__costFunction(mapping))\n",
    "    \n",
    "\n",
    "    def readCSV(self,fileName=mapping_file_path):\n",
    "        self.__reset()\n",
    "        with open(fileName,'r') as csvfile:\n",
    "            mappingReader = csv.reader(csvfile)\n",
    "            next(mappingReader)\n",
    "            for line in mappingReader:\n",
    "                newMapping:list[int]=[]\n",
    "                for v in line:\n",
    "                    if v.isdigit():\n",
    "                        newMapping.append(int(v))\n",
    "                    else:\n",
    "                        self.costs.append(float(v))\n",
    "\n",
    "                self.mappings.append(newMapping)\n",
    "\n",
    "\n",
    "    def writeCSV(self,filepath =mapping_file_path):\n",
    "        with open(filepath,'w',newline='') as csvfile:\n",
    "                mappingWriter = csv.writer(csvfile)\n",
    "                mappingWriter.writerow([\"T1\",'T2','T3','T4','T5','T6','T7','T8','T9','T10','Penalty'])\n",
    "                mappingWriter.writerows(self.mappings)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def testDataGen():\n",
    "    dl = DataLoader()\n",
    "    dl.loadAll()\n",
    "    mappingloader = MappingHandler(dl.tasks,dl.employees)\n",
    "    mappingloader.generateMappings()\n",
    "    mappingloader.writeCSV('data/Mappings.csv')\n",
    "\n",
    "\n",
    "testDataGen()\n",
    "\n",
    "\n",
    "# def TestDataGen():\n",
    "\n",
    "#     all_mappings = mappingGenerator(task,employees)\n",
    "#     for mapping in all_mappings:\n",
    "#         cost = costFunction(mapping,task,employees)\n",
    "#         print(cost)\n",
    "\n",
    "# def verifyCost():\n",
    "#     mapping=[[2],[3],[1],[4],[2],[5],[1],[3],[5],[4]]\n",
    "    \n",
    "#     for i in range(len(mapping)):\n",
    "#         mapping[i][0]-=1\n",
    "#     employees,tasks = DataLoader().loadAll()\n",
    "#     # for t in tasks:\n",
    "#     #     print(t)\n",
    "    \n",
    "#     for e in employees:\n",
    "#         print(e)\n",
    "#     print(costFunction(mapping,tasks,employees))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "18263f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Preprocessing\n",
    "# returns a 3-element vector\n",
    "def one_hot_encode_skill(skillsVector):\n",
    "    skills= ['A','B','C']\n",
    "    hot_encoding=[]\n",
    "    j=0\n",
    "    for i in range(len(skills)):\n",
    "        if j< len(skillsVector)and  skills[i] == skillsVector[j]:\n",
    "            j+=1\n",
    "            hot_encoding.append(1)\n",
    "        else:\n",
    "            hot_encoding.append(0)\n",
    "    return hot_encoding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def construct_input_vector(mappingloader:MappingHandler):\n",
    "    featureVector = []\n",
    "\n",
    "    for mp in mappingloader.mappings:\n",
    "        vector=[]\n",
    "        for i in range(len(mp)):\n",
    "\n",
    "            task = mappingloader.tasks[i]\n",
    "            employee = mappingloader.employees[mp[i]]\n",
    "            vector.extend([task.time,task.difficulty,task.deadline])\n",
    "            vector.extend(one_hot_encode_skill(task.required_skill))\n",
    "            vector.extend([employee.available_hours,employee.skill_level])\n",
    "            vector.extend(one_hot_encode_skill( employee.skills))\n",
    "            vector.append(mappingloader.costs[i])\n",
    "        featureVector.append(vector)\n",
    "        \n",
    "    df = pd.DataFrame(featureVector)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_train_val_test(df:pd.DataFrame):\n",
    "    df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    train_size = int(0.7 * len(df_shuffled))\n",
    "    val_size = int(0.15 * len(df_shuffled))\n",
    "\n",
    "    train_df = df_shuffled[:train_size]\n",
    "    val_df = df_shuffled[train_size:train_size + val_size]\n",
    "    test_df = df_shuffled[train_size + val_size:]\n",
    "    return  train_df,val_df,test_df\n",
    "\n",
    "def split_x_y(df:pd.DataFrame):\n",
    "    x =df[:-1]\n",
    "    y=  df[-1]\n",
    "    return x,y\n",
    "\n",
    "\n",
    "def create_batches(data:pd.DataFrame, batch_size:int):\n",
    "    return [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "\n",
    "\n",
    "def pre_process():\n",
    "    dl = DataLoader()\n",
    "    dl.loadAll()\n",
    "    mappingloader = MappingHandler(dl.tasks,dl.employees)\n",
    "    mappingloader.readCSV('data/Mappings.csv')\n",
    "    df = construct_input_vector(mappingloader)\n",
    "    return split_train_val_test(df)\n",
    "\n",
    "# pre_process()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5414f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model Definitions\n",
    "\n",
    "import math\n",
    "def sig(x):\n",
    "    return 1/(1+np.exp(-np.clip(x, -500, 500)))  # Clip to prevent overflow\n",
    "\n",
    "def sig_derivative(x):\n",
    "    s = sig(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def linear(x):\n",
    "\n",
    "    return x\n",
    "\n",
    "def linear_derivative(x):\n",
    "    return np.ones_like(x)\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "\n",
    "\n",
    "class NeuralNetwrokArgs:\n",
    "    layer_dims=[]\n",
    "    name:str=''\n",
    "    activation:Callable[[int],int]=relu\n",
    "    activation_derivative:Callable[[int],int] = relu_derivative\n",
    "    lr=0.01\n",
    "    output_activation:Callable[[int],int] = linear\n",
    "    output_activation_derivative = linear_derivative\n",
    "    epochs=100\n",
    "    batch_size=16\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, neuralArgs:NeuralNetwrokArgs):\n",
    "\n",
    "        self.name = neuralArgs.name\n",
    "        self.layer_dims= neuralArgs.layer_dims\n",
    "        self.weights = np.array([])\n",
    "        self.biases= np.array([])\n",
    "        self.activation = neuralArgs.activation\n",
    "        self.activation_derivative = neuralArgs.activation_derivative\n",
    "        self.output_activation = neuralArgs.output_activation\n",
    "        self.output_activation_derivative = neuralArgs.output_activation\n",
    "        self.lr= neuralArgs.lr\n",
    "        self.batch_size = neuralArgs.batch_size\n",
    "        self.epochs = neuralArgs.epochs\n",
    "\n",
    "\n",
    "        for i in range(1,len(self.layer_dims)):\n",
    "            w = np.random.randn(self.layer_dims[i],self.layer_dims[i-1])*0.01\n",
    "            self.weights= np.append(self.weights,w)\n",
    "            b= np.zeros((self.layer_dims[i],1))\n",
    "            self.biases =np.append(self.biases,b)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        cache_a=[x]\n",
    "        cache_z=[]\n",
    "        for i in range(len(self.layer_dims)-1):\n",
    "            z=self.weights[i].dot(cache_a[i])+self.biases[i]\n",
    "\n",
    "            if i == len(self.layer_dims) - 2:  # Output layer\n",
    "                a = self.output_activation(z)\n",
    "            else:\n",
    "                a = self.activation(z)\n",
    "            cache_a.append(a)\n",
    "            cache_z.append(z)\n",
    "        return cache_a,cache_z\n",
    "\n",
    "    def backward(self, x, y_true,cache_a,cache_z):\n",
    "        grads={}\n",
    "        deltas=[]\n",
    "\n",
    "        #Output layer\n",
    "        delta_output = 2* (cache_a[-1]-y_true) * self.output_activation_derivative(cache_z[-1])\n",
    "        deltas.append(delta_output)\n",
    "\n",
    "        #hidden layers\n",
    "        current_delta = delta_output\n",
    "\n",
    "        for l in range(len(self.layer_dims)-2,0,-1):\n",
    "            delta_hidden = self.weights[l].T.dot(current_delta) * self.activation_derivative(cache_z[l-1])\n",
    "            deltas.insert(0, delta_hidden)\n",
    "            current_delta = delta_hidden\n",
    "\n",
    "        for l in range(len(self.weights)):\n",
    "            # ∂L/∂W^(l) = δ^(l) (a^(l-1))^T\n",
    "            dW = deltas[l].dot(cache_a[l].T)\n",
    "            \n",
    "            # ∂L/∂b^(l) = δ^(l)\n",
    "            db = np.sum(deltas[l], axis=1, keepdims=True)\n",
    "            \n",
    "            grads[f'dW{l+1}'] = dW\n",
    "            grads[f'db{l+1}'] = db\n",
    "        \n",
    "        return grads\n",
    "\n",
    "\n",
    "\n",
    "    def update_params(self,grads):\n",
    "        for l in range(len(self.weights)):\n",
    "            # W^(l) ← W^(l) - α * ∂L/∂W^(l)\n",
    "            self.weights[l] -= self.lr * grads[f'dW{l+1}']\n",
    "            \n",
    "            # b^(l) ← b^(l) - α * ∂L/∂b^(l)\n",
    "            self.biases[l] -= self.lr * grads[f'db{l+1}']\n",
    "\n",
    "    # 6. Training Loop\n",
    "    def train(self, trainData:pd.DataFrame):\n",
    "        for e in range(self.epochs):\n",
    "            trainData = trainData.sample(frac=1).reset_index(drop=True)\n",
    "            batches = create_batches(trainData,self.batch_size)\n",
    "            for batch in batches:\n",
    "                x_train,y_train =split_x_y(batch)\n",
    "                cache_a,cache_z = self.forward(x_train)\n",
    "                mse(y_train,cache_a[-1])\n",
    "                grads = self.backward(x_train,y_train,cache_a,cache_z)\n",
    "                self.update_params(grads)\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "    # df_x= df_shuffled[:-1]\n",
    "    # df_y=  df_shuffled[-1]\n",
    "        \n",
    "    # implement mini-batch SGD, record loss\n",
    "\n",
    "# 7. Evaluation & Plots\n",
    "#- Generate the eight required figures\n",
    "#- Save each via plt.savefig()\n",
    "# 8. Save & Export\n",
    "#- Download figures\n",
    "#- Optionally, pickle model parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
