{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "543cf45a",
   "metadata": {},
   "source": [
    "# 1. Notebook Setup\n",
    "# Title, assignment info, and markdown overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "044f2c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "import csv\n",
    "from typing import Union\n",
    "from typing import Callable\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "48957c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_file_path = \"data/Mappings.csv\"\n",
    "employees_file_path = \"data/Employee.csv\"\n",
    "tasks_file_path = \"data/Tasks.csv\"\n",
    "rd.seed(20)\n",
    "\n",
    "# Data Classes\n",
    "class Task:\n",
    "    time:int =0\n",
    "    difficulty:int =0\n",
    "    deadline:int = 0\n",
    "    required_skill:str = ''\n",
    "    def __str__(self):\n",
    "        return f\"T Time:{self.time} Difficulty:{self.difficulty} Deadline:{self.deadline} requiredSkill:{self.required_skill}\"\n",
    "\n",
    "class Employee:\n",
    "    available_hours:int = 0\n",
    "    skill_level:int = 4\n",
    "    skills:list[str]=['']\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"E AH:{self.available_hours} Skill-Level:{self.skill_level} Skills:{self.skills}\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7ea3c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O\n",
    "\"\"\"\n",
    "Task csv:\n",
    "\n",
    "'ID', 'Time (hrs)', 'Difficulty', 'Deadline (hrs)', 'Required Skill'\n",
    "\n",
    "Employee csv:\n",
    "'Employee ID', 'Available Hrs', 'Skill Level', 'Skills'\n",
    "\n",
    "\n",
    "Using an adjancey list instead of an adjacney matrix!\n",
    "\n",
    "Since the input vector is 10 mappings x 11 features. Including a unique assignment penalty is redundant since a task mapped to 2 employees will create (10+1) mappings which doesn't work for the network input layer!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    num_of_tasks =10\n",
    "    num_of_employees = 5\n",
    "    tasks:list[Task]=[]\n",
    "    employees:list[Employee]=[]\n",
    "    \n",
    "    def load_tasks(self,fileName=tasks_file_path):\n",
    "        tasks =[]\n",
    "        with open(fileName,'r') as csvfile:\n",
    "            taskReader = csv.reader(csvfile)\n",
    "            next(taskReader)\n",
    "            for taskArr in taskReader:\n",
    "                newT = Task()\n",
    "                newT.time=int(taskArr[1])\n",
    "                newT.difficulty= int(taskArr[2])\n",
    "                newT.deadline = int(taskArr[3])\n",
    "                newT.required_skill= taskArr[4]\n",
    "                tasks.append(newT)\n",
    "        self.tasks= tasks\n",
    "\n",
    "    def load_employees(self,fileName=employees_file_path):\n",
    "        employees=[]\n",
    "        with open(fileName,'r') as csvfile:\n",
    "            employeeReader = csv.reader(csvfile)\n",
    "            next(employeeReader)\n",
    "            for employeeArr in employeeReader:\n",
    "                newEmployee = Employee()\n",
    "                newEmployee.available_hours=int(employeeArr[1])\n",
    "                newEmployee.skill_level= int(employeeArr[2])\n",
    "                skills = employeeArr[3].split(',')\n",
    "                newEmployee.skills = skills\n",
    "                employees.append(newEmployee)\n",
    "        self.employees = employees\n",
    "    def loadAll(self):\n",
    "        self.load_employees()\n",
    "        self.load_tasks()\n",
    "    \n",
    "\n",
    "\n",
    "class MappingHandler:\n",
    "    def __init__(self,tasks:list[Task],employees:list[Employee],num_of_mappings=100):\n",
    "        self.tasks:list[Task]= tasks\n",
    "        self.employees:list[Employee] = employees\n",
    "        self.num_of_mappings = num_of_mappings\n",
    "        self.mappings:list[list[int]] = []\n",
    "        self.costs:list[float]=[]\n",
    "\n",
    "\n",
    "    def __costFunction(self,Mapping:list[int]):\n",
    "        w=0.2\n",
    "        overload = 0\n",
    "        skill_mismatch = 0\n",
    "        difficulty_violation = 0\n",
    "        deadline_violation = 0\n",
    "        unique_assignment = 0\n",
    "        employee_task_adj_list:list[list[Task]]=[[] for _ in range(len(self.employees))]\n",
    "\n",
    "        for i in range(len(Mapping)):\n",
    "            taskId=i\n",
    "            task = self.tasks[taskId]\n",
    "            num_of_employees_assigned = 0\n",
    "            employeeId = Mapping[i]\n",
    "            num_of_employees_assigned+=1\n",
    "            employee = self.employees[employeeId]\n",
    "            employee_task_adj_list[employeeId].append(task)\n",
    "            # skill mismatch violation\n",
    "            if task.required_skill not in employee.skills:\n",
    "                skill_mismatch+=1\n",
    "            if task.difficulty> employee.skill_level:\n",
    "                difficulty_violation+= 1\n",
    "            unique_assignment+= max(0,num_of_employees_assigned-1)\n",
    "\n",
    "\n",
    "\n",
    "        for employeeId in range(len(employee_task_adj_list)):\n",
    "            employee = self.employees[employeeId]\n",
    "            sortedEmployeeTasks = employee_task_adj_list[employeeId]\n",
    "            sortedEmployeeTasks.sort(key= lambda x:x.time)\n",
    "            sumHours=0\n",
    "            finishTime=0\n",
    "            for t in sortedEmployeeTasks:\n",
    "                sumHours+=t.time\n",
    "                finishTime+= t.time\n",
    "                deadline_violation+= max(0,finishTime - t.deadline)\n",
    "            overload+= max(0,sumHours- employee.available_hours) \n",
    "        total_penalty = overload+skill_mismatch+unique_assignment+deadline_violation+ difficulty_violation\n",
    "        return round(total_penalty*w,3)\n",
    "    def __reset(self):\n",
    "        self.mappings=[]\n",
    "        self.costs=[]\n",
    "\n",
    "\n",
    "    def generateMappings(self):\n",
    "        unique = set()\n",
    "        self.__reset()\n",
    "        while len(unique) < self.num_of_mappings:\n",
    "            mapping:list[int] = [0 for _ in self.tasks]         # Create a mapping for each iteration. Using an adjlist format\n",
    "            # possible_assignments = [i for i in range(len(employees))]  #list of possible assignments\n",
    "            for taskId in range(len(self.tasks)):\n",
    "                rand_employee =rd.randint(0,len(self.employees)-1)\n",
    "                mapping[taskId]=rand_employee\n",
    "            string = str(mapping)\n",
    "            if string not in unique: # ensure no duplicates in data generation\n",
    "                unique.add(string)\n",
    "                self.mappings.append(mapping)\n",
    "                self.costs.append(self.__costFunction(mapping))\n",
    "    \n",
    "\n",
    "    def readCSV(self,fileName=mapping_file_path):\n",
    "        self.__reset()\n",
    "        with open(fileName,'r') as csvfile:\n",
    "            mappingReader = csv.reader(csvfile)\n",
    "            next(mappingReader)\n",
    "            for line in mappingReader:\n",
    "                newMapping:list[int]=[]\n",
    "                for v in line:\n",
    "                    if v.isdigit():\n",
    "                        newMapping.append(int(v))\n",
    "                    else:\n",
    "                        self.costs.append(float(v))\n",
    "\n",
    "                self.mappings.append(newMapping)\n",
    "\n",
    "\n",
    "    def writeCSV(self,filepath =mapping_file_path):\n",
    "        with open(filepath,'w',newline='') as csvfile:\n",
    "                mappingWriter = csv.writer(csvfile,delimiter=',')\n",
    "                mappingWriter.writerow([\"T1\",'T2','T3','T4','T5','T6','T7','T8','T9','T10','Penalty'])\n",
    "                for i in range(len(self.mappings)):\n",
    "                    row=[]\n",
    "                    row.extend(self.mappings[i])\n",
    "                    row.append(f\"{self.costs[i]}\")\n",
    "                    mappingWriter.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def testDataGen():\n",
    "    dl = DataLoader()\n",
    "    dl.loadAll()\n",
    "    mappingloader = MappingHandler(dl.tasks,dl.employees)\n",
    "    mappingloader.generateMappings()\n",
    "    mappingloader.writeCSV('data/Mappings.csv')\n",
    "\n",
    "\n",
    "testDataGen()\n",
    "\n",
    "\n",
    "# def TestDataGen():\n",
    "\n",
    "#     all_mappings = mappingGenerator(task,employees)\n",
    "#     for mapping in all_mappings:\n",
    "#         cost = costFunction(mapping,task,employees)\n",
    "#         print(cost)\n",
    "\n",
    "# def verifyCost():\n",
    "#     mapping=[[2],[3],[1],[4],[2],[5],[1],[3],[5],[4]]\n",
    "    \n",
    "#     for i in range(len(mapping)):\n",
    "#         mapping[i][0]-=1\n",
    "#     employees,tasks = DataLoader().loadAll()\n",
    "#     # for t in tasks:\n",
    "#     #     print(t)\n",
    "    \n",
    "#     for e in employees:\n",
    "#         print(e)\n",
    "#     print(costFunction(mapping,tasks,employees))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "18263f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    0    1    2    3    4    5    6    7    8    9    ...  101  102  103  104  \\\n",
       " 0     4    3    8    1    0    0    8    3    1    0  ...   11    0    0    1   \n",
       " 1     4    3    8    1    0    0    8    3    1    0  ...   11    0    0    1   \n",
       " 2     4    3    8    1    0    0   10    4    1    0  ...   11    0    0    1   \n",
       " 3     4    3    8    1    0    0   10    4    1    0  ...   11    0    0    1   \n",
       " 4     4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " ..  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       " 65    4    3    8    1    0    0   15    7    0    1  ...   11    0    0    1   \n",
       " 66    4    3    8    1    0    0    8    3    1    0  ...   11    0    0    1   \n",
       " 67    4    3    8    1    0    0   10    4    1    0  ...   11    0    0    1   \n",
       " 68    4    3    8    1    0    0   12    6    1    1  ...   11    0    0    1   \n",
       " 69    4    3    8    1    0    0   15    7    0    1  ...   11    0    0    1   \n",
       " \n",
       "     105  106  107  108  109  110  \n",
       " 0    10    4    1    0    1  3.6  \n",
       " 1     9    5    1    0    1  3.8  \n",
       " 2     9    5    1    0    1  3.4  \n",
       " 3     9    5    1    0    1  3.0  \n",
       " 4    10    4    1    0    1  4.6  \n",
       " ..  ...  ...  ...  ...  ...  ...  \n",
       " 65   12    6    1    1    1  5.0  \n",
       " 66   12    6    1    1    1  7.0  \n",
       " 67   12    6    1    1    1  4.4  \n",
       " 68   15    7    0    1    1  5.6  \n",
       " 69   10    4    1    0    1  3.0  \n",
       " \n",
       " [70 rows x 111 columns],\n",
       "     0    1    2    3    4    5    6    7    8    9    ...  101  102  103  104  \\\n",
       " 70    4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " 71    4    3    8    1    0    0   12    6    1    1  ...   11    0    0    1   \n",
       " 72    4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " 73    4    3    8    1    0    0   10    4    1    0  ...   11    0    0    1   \n",
       " 74    4    3    8    1    0    0    8    3    1    0  ...   11    0    0    1   \n",
       " 75    4    3    8    1    0    0   10    4    1    0  ...   11    0    0    1   \n",
       " 76    4    3    8    1    0    0   15    7    0    1  ...   11    0    0    1   \n",
       " 77    4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " 78    4    3    8    1    0    0   12    6    1    1  ...   11    0    0    1   \n",
       " 79    4    3    8    1    0    0   10    4    1    0  ...   11    0    0    1   \n",
       " 80    4    3    8    1    0    0    8    3    1    0  ...   11    0    0    1   \n",
       " 81    4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " 82    4    3    8    1    0    0   12    6    1    1  ...   11    0    0    1   \n",
       " 83    4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " 84    4    3    8    1    0    0   10    4    1    0  ...   11    0    0    1   \n",
       " \n",
       "     105  106  107  108  109  110  \n",
       " 70   10    4    1    0    1  4.0  \n",
       " 71    8    3    1    0    0  2.2  \n",
       " 72    9    5    1    0    1  6.4  \n",
       " 73    8    3    1    0    0  7.8  \n",
       " 74   10    4    1    0    1  5.4  \n",
       " 75    9    5    1    0    1  2.6  \n",
       " 76   12    6    1    1    1  2.8  \n",
       " 77    9    5    1    0    1  3.8  \n",
       " 78   10    4    1    0    1  8.0  \n",
       " 79   10    4    1    0    1  5.8  \n",
       " 80   12    6    1    1    1  7.0  \n",
       " 81   12    6    1    1    1  5.8  \n",
       " 82   12    6    1    1    1  2.4  \n",
       " 83   15    7    0    1    1  2.8  \n",
       " 84    8    3    1    0    0  2.0  \n",
       " \n",
       " [15 rows x 111 columns],\n",
       "     0    1    2    3    4    5    6    7    8    9    ...  101  102  103  104  \\\n",
       " 85    4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " 86    4    3    8    1    0    0   10    4    1    0  ...   11    0    0    1   \n",
       " 87    4    3    8    1    0    0    8    3    1    0  ...   11    0    0    1   \n",
       " 88    4    3    8    1    0    0   15    7    0    1  ...   11    0    0    1   \n",
       " 89    4    3    8    1    0    0    8    3    1    0  ...   11    0    0    1   \n",
       " 90    4    3    8    1    0    0   12    6    1    1  ...   11    0    0    1   \n",
       " 91    4    3    8    1    0    0   12    6    1    1  ...   11    0    0    1   \n",
       " 92    4    3    8    1    0    0   15    7    0    1  ...   11    0    0    1   \n",
       " 93    4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " 94    4    3    8    1    0    0   15    7    0    1  ...   11    0    0    1   \n",
       " 95    4    3    8    1    0    0   15    7    0    1  ...   11    0    0    1   \n",
       " 96    4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " 97    4    3    8    1    0    0   12    6    1    1  ...   11    0    0    1   \n",
       " 98    4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " 99    4    3    8    1    0    0    9    5    1    0  ...   11    0    0    1   \n",
       " \n",
       "     105  106  107  108  109  110  \n",
       " 85   12    6    1    1    1  5.6  \n",
       " 86    8    3    1    0    0  4.6  \n",
       " 87    9    5    1    0    1  1.8  \n",
       " 88    8    3    1    0    0  7.0  \n",
       " 89   15    7    0    1    1  5.2  \n",
       " 90   10    4    1    0    1  3.2  \n",
       " 91   15    7    0    1    1  2.2  \n",
       " 92   12    6    1    1    1  2.2  \n",
       " 93   12    6    1    1    1  3.0  \n",
       " 94    8    3    1    0    0  4.4  \n",
       " 95   10    4    1    0    1  3.4  \n",
       " 96   10    4    1    0    1  7.2  \n",
       " 97   15    7    0    1    1  5.2  \n",
       " 98    8    3    1    0    0  6.8  \n",
       " 99    8    3    1    0    0  3.2  \n",
       " \n",
       " [15 rows x 111 columns])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Preprocessing\n",
    "# returns a 3-element vector\n",
    "def one_hot_encode_skill(skillsVector):\n",
    "    skills= ['A','B','C']\n",
    "    hot_encoding=[]\n",
    "    j=0\n",
    "    for i in range(len(skills)):\n",
    "        if j< len(skillsVector)and  skills[i] == skillsVector[j]:\n",
    "            j+=1\n",
    "            hot_encoding.append(1)\n",
    "        else:\n",
    "            hot_encoding.append(0)\n",
    "    return hot_encoding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def construct_input_vector(mappingloader:MappingHandler):\n",
    "    featureVector = []\n",
    "\n",
    "    for idx,mp in  enumerate(mappingloader.mappings):\n",
    "        vector=[]\n",
    "        for i in range(len(mp)):\n",
    "\n",
    "            task = mappingloader.tasks[i]\n",
    "            employee = mappingloader.employees[mp[i]]\n",
    "            vector.extend([task.time,task.difficulty,task.deadline])\n",
    "            vector.extend(one_hot_encode_skill(task.required_skill))\n",
    "            vector.extend([employee.available_hours,employee.skill_level])\n",
    "            vector.extend(one_hot_encode_skill( employee.skills))\n",
    "        vector.append(mappingloader.costs[idx])\n",
    "        featureVector.append(vector)\n",
    "        \n",
    "    df = DataFrame(featureVector)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_train_val_test(df:DataFrame):\n",
    "    df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    train_size = int(0.7 * len(df_shuffled))\n",
    "    val_size = int(0.15 * len(df_shuffled))\n",
    "\n",
    "    train_df = df_shuffled[:train_size]\n",
    "    val_df = df_shuffled[train_size:train_size + val_size]\n",
    "    test_df = df_shuffled[train_size + val_size:]\n",
    "    return  train_df,val_df,test_df\n",
    "\n",
    "def split_x_y(df:DataFrame):\n",
    "\n",
    "    x = df.iloc[:, :-1]\n",
    "\n",
    "    y = df.iloc[:, -1]  \n",
    "    return x,y\n",
    "\n",
    "\n",
    "def create_batches(data:DataFrame, batch_size:int):\n",
    "    batches=[]\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batches.append(data[i:i + batch_size])\n",
    "    return batches\n",
    "\n",
    "\n",
    "def pre_process():\n",
    "    dl = DataLoader()\n",
    "    dl.loadAll()\n",
    "    mappingloader = MappingHandler(dl.tasks,dl.employees)\n",
    "    mappingloader.readCSV('data/Mappings.csv')\n",
    "    df = construct_input_vector(mappingloader)\n",
    "    return split_train_val_test(df)\n",
    "\n",
    "pre_process()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5414f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16,)\n",
      "(16,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (1) does not match length of index (16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[192], line 168\u001b[0m\n\u001b[0;32m    164\u001b[0m     nn \u001b[38;5;241m=\u001b[39m NeuralNetwork(args)\n\u001b[0;32m    165\u001b[0m     nn\u001b[38;5;241m.\u001b[39mtrain(trainData,valData)\n\u001b[1;32m--> 168\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# 7. Evaluation & Plots\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m#- Generate the eight required figures\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m#- Save each via plt.savefig()\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# 8. Save & Export\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m#- Download figures\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m#- Optionally, pickle model parameters\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[192], line 165\u001b[0m, in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m    163\u001b[0m args \u001b[38;5;241m=\u001b[39m NeuralNetwrokArgs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    164\u001b[0m nn \u001b[38;5;241m=\u001b[39m NeuralNetwork(args)\n\u001b[1;32m--> 165\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainData\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalData\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[192], line 145\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[1;34m(self, trainData, valData)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mprint\u001b[39m(y_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# print(mse(y_train,cache_a[-1]))\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcache_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcache_z\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_params(grads)\n\u001b[0;32m    147\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[192], line 95\u001b[0m, in \u001b[0;36mNeuralNetwork.backward\u001b[1;34m(self, y_true, cache_a, cache_z)\u001b[0m\n\u001b[0;32m     92\u001b[0m deltas\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m#Output layer\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m delta_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_a\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_activation_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_z\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m deltas\u001b[38;5;241m.\u001b[39mappend(delta_output)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m#hidden layers\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yousf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yousf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arraylike.py:202\u001b[0m, in \u001b[0;36mOpsMixin.__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__mul__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__mul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yousf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:6135\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[0;32m   6134\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other)\n\u001b[1;32m-> 6135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexOpsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yousf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:1384\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1382\u001b[0m     result \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39marithmetic_op(lvalues, rvalues, op)\n\u001b[1;32m-> 1384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yousf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:6231\u001b[0m, in \u001b[0;36mSeries._construct_result\u001b[1;34m(self, result, name)\u001b[0m\n\u001b[0;32m   6228\u001b[0m \u001b[38;5;66;03m# TODO: result should always be ArrayLike, but this fails for some\u001b[39;00m\n\u001b[0;32m   6229\u001b[0m \u001b[38;5;66;03m#  JSONArray tests\u001b[39;00m\n\u001b[0;32m   6230\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 6231\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   6232\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   6234\u001b[0m \u001b[38;5;66;03m# Set the result's name after __finalize__ is called because __finalize__\u001b[39;00m\n\u001b[0;32m   6235\u001b[0m \u001b[38;5;66;03m#  would set it back to self.name\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yousf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:575\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    573\u001b[0m     index \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n\u001b[1;32m--> 575\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;66;03m# create/copy the manager\u001b[39;00m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (SingleBlockManager, SingleArrayManager)):\n",
      "File \u001b[1;32mc:\\Users\\yousf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (1) does not match length of index (16)"
     ]
    }
   ],
   "source": [
    "# 5. Model Definitions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sig(x):\n",
    "    return 1/(1+np.exp(-np.clip(x, -500, 500)))  # Clip to prevent overflow\n",
    "\n",
    "def sig_derivative(x):\n",
    "    s = sig(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_derivative(x):\n",
    "    return np.ones_like(x)\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.sum(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "architecture={\n",
    "    'A':[110,256,1],\n",
    "    'B':[110,128,128,1]\n",
    "}\n",
    "\n",
    "class NeuralNetwrokArgs:\n",
    "\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        self.layer_dims = architecture[self.name]\n",
    "        self.activation:Callable[[np.ndarray],np.ndarray]=relu\n",
    "        self.activation_derivative:Callable[[np.ndarray],np.ndarray] = relu_derivative\n",
    "        self.lr=0.01\n",
    "        self.output_activation:Callable[[np.ndarray],np.ndarray] = linear\n",
    "        self.output_activation_derivative = linear_derivative\n",
    "        self.epochs=100\n",
    "        self.batch_size=16\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, neuralArgs:NeuralNetwrokArgs):\n",
    "\n",
    "        self.name = neuralArgs.name\n",
    "        self.layer_dims= neuralArgs.layer_dims\n",
    "        self.weights = []\n",
    "        self.biases= []\n",
    "        self.activation = neuralArgs.activation\n",
    "        self.activation_derivative = neuralArgs.activation_derivative\n",
    "        self.output_activation = neuralArgs.output_activation\n",
    "        self.output_activation_derivative = neuralArgs.output_activation\n",
    "        self.lr= neuralArgs.lr\n",
    "        self.batch_size = neuralArgs.batch_size\n",
    "        self.epochs = neuralArgs.epochs\n",
    "\n",
    "\n",
    "        for i in range(1,len(self.layer_dims)):\n",
    "            w = np.random.randn(self.layer_dims[i],self.layer_dims[i-1])*0.01\n",
    "            self.weights.append(w)\n",
    "            b= np.zeros((self.layer_dims[i],1))\n",
    "            self.biases.append(b)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        cache_a=[x.T]\n",
    "        cache_z=[]\n",
    "        for i in range(len(self.layer_dims)-1):\n",
    "            z=self.weights[i].dot(cache_a[i])+self.biases[i]\n",
    "            if i == len(self.layer_dims) - 2:  # Output layer\n",
    "                a = self.output_activation(z)\n",
    "            else:\n",
    "                a = self.activation(z)\n",
    "            cache_a.append(a)\n",
    "            cache_z.append(z)\n",
    "        return cache_a,cache_z\n",
    "\n",
    "    def backward(self,y_true,cache_a,cache_z):\n",
    "        grads={}\n",
    "        deltas=[]\n",
    "\n",
    "        #Output layer\n",
    "        delta_output = 2* (cache_a[-1]-y_true) * self.output_activation_derivative(cache_z[-1])\n",
    "        deltas.append(delta_output)\n",
    "\n",
    "        #hidden layers\n",
    "        current_delta = delta_output\n",
    "\n",
    "        for l in range(len(self.layer_dims)-2,0,-1):\n",
    "            delta_hidden = self.weights[l].T.dot(current_delta) * self.activation_derivative(cache_z[l-1])\n",
    "            deltas.insert(0, delta_hidden)\n",
    "            current_delta = delta_hidden\n",
    "\n",
    "        for l in range(len(self.weights)):\n",
    "            # ∂L/∂W^(l) = δ^(l) (a^(l-1))^T\n",
    "            dW = deltas[l].dot(cache_a[l].T)\n",
    "            \n",
    "            # ∂L/∂b^(l) = δ^(l)\n",
    "            db = np.sum(deltas[l], axis=1, keepdims=True)\n",
    "            \n",
    "            grads[f'dW{l+1}'] = dW\n",
    "            grads[f'db{l+1}'] = db\n",
    "        \n",
    "        return grads\n",
    "\n",
    "\n",
    "\n",
    "    def update_params(self,grads):\n",
    "        for l in range(len(self.weights)):\n",
    "            # W^(l) ← W^(l) - α * ∂L/∂W^(l)\n",
    "            self.weights[l] -= self.lr * grads[f'dW{l+1}']\n",
    "            \n",
    "            # b^(l) ← b^(l) - α * ∂L/∂b^(l)\n",
    "            self.biases[l] -= self.lr * grads[f'db{l+1}']\n",
    "    \n",
    "\n",
    "\n",
    "    # 6. Training Loop\n",
    "    def train(self, trainData:DataFrame,valData:DataFrame):\n",
    "\n",
    "        for e in range(self.epochs):\n",
    "            start = time.time()\n",
    "            trainData = trainData.sample(frac=1).reset_index(drop=True)\n",
    "            batches = create_batches(trainData,self.batch_size)\n",
    "            train_loss =0\n",
    "            for batch in batches:\n",
    "                x_train,y_train =split_x_y(batch)\n",
    "                cache_a,cache_z = self.forward(x_train)\n",
    "                print(cache_a[-1].shape)\n",
    "                print(y_train.shape)\n",
    "                # print(mse(y_train,cache_a[-1]))\n",
    "\n",
    "                grads = self.backward(y_train,cache_a,cache_z)\n",
    "                self.update_params(grads)\n",
    "            end = time.time()\n",
    "            val_pred, val_loss = self.evaluate(valData)\n",
    "            print(f\"Epoch: {e}   time: {end-start}   train_loss: {train_loss}   val_loss: {val_loss}\")\n",
    "\n",
    "\n",
    "    def evaluate(self,evalData):\n",
    "        x,y_true =split_x_y(evalData)\n",
    "        cache_a,cache_z = self.forward(x)\n",
    "        return cache_a[-1],mse(y_true,cache_a[-1])\n",
    "\n",
    "    def predict(self,predData):\n",
    "        return self.forward(predData)[-1]\n",
    "\n",
    "\n",
    "def test():\n",
    "    trainData,valData,testData = pre_process()\n",
    "    args = NeuralNetwrokArgs('A')\n",
    "    nn = NeuralNetwork(args)\n",
    "    nn.train(trainData,valData)\n",
    "\n",
    "\n",
    "test()\n",
    "\n",
    "# 7. Evaluation & Plots\n",
    "#- Generate the eight required figures\n",
    "#- Save each via plt.savefig()\n",
    "# 8. Save & Export\n",
    "#- Download figures\n",
    "#- Optionally, pickle model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de29bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class HyperArgs:\n",
    "    names=['A','B']\n",
    "    lr=[0.01, 0.001, 0.0001]\n",
    "    batch_size=[8,16,32]\n",
    "    epochs=[100,150,200]\n",
    "    activations=[[sig,sig_derivative],[relu,relu_derivative]]\n",
    "\n",
    "def hyper_grid_search(gridArgs:HyperArgs):\n",
    "    trainData,valData,testData = pre_process()\n",
    "    for name in gridArgs.names:\n",
    "        for lr in gridArgs.lr:\n",
    "            for batch in gridArgs.batch_size:\n",
    "                for epoch in gridArgs.epochs:\n",
    "                    for activation in gridArgs.activations:\n",
    "                        args = NeuralNetwrokArgs()\n",
    "                        args.lr = lr\n",
    "                        args.batch_size = batch\n",
    "                        args.epochs = epoch\n",
    "                        args.activation = activation[0]\n",
    "                        args.activation_derivative = activation[1]\n",
    "                        args.name = name\n",
    "                        args.layer_dims = architecture[name]\n",
    "                        nn= NeuralNetwork(args)\n",
    "                        nn.train(trainData,valData)\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa2142",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m     nn \u001b[38;5;241m=\u001b[39m NeuralNetwork(args)\n\u001b[0;32m      5\u001b[0m     nn\u001b[38;5;241m.\u001b[39mtrain(trainData,valData)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[142], line 5\u001b[0m, in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m args \u001b[38;5;241m=\u001b[39m NeuralNetwrokArgs()\n\u001b[0;32m      4\u001b[0m nn \u001b[38;5;241m=\u001b[39m NeuralNetwork(args)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainData\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalData\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[137], line 138\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[1;34m(self, trainData, valData)\u001b[0m\n\u001b[0;32m    136\u001b[0m     cache_a,cache_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x_train)\n\u001b[0;32m    137\u001b[0m     train_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mmse(y_train,cache_a[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m--> 138\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcache_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcache_z\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_params(grads)\n\u001b[0;32m    140\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[137], line 91\u001b[0m, in \u001b[0;36mNeuralNetwork.backward\u001b[1;34m(self, x, y_true, cache_a, cache_z)\u001b[0m\n\u001b[0;32m     88\u001b[0m deltas\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m#Output layer\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m delta_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m (cache_a[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39my_true) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_activation_derivative(\u001b[43mcache_z\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     92\u001b[0m deltas\u001b[38;5;241m.\u001b[39mappend(delta_output)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m#hidden layers\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
